name: ollama smoke tests
on:
    workflow_dispatch:
    release:
        types:
            - published
    pull_request:
        paths:
            - yarn.lock
            - ".github/workflows/ollama.yml"
            - "packages/core/**/*"
            - "packages/cli/**/*"
            - "packages/samples/**/*"
concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}-ollama
    cancel-in-progress: true
env:
    GENAISCRIPT_DEFAULT_REASONING_MODEL: ${{ vars.GENAISCRIPT_DEFAULT_REASONING_MODEL }}
    GENAISCRIPT_DEFAULT_REASONING_SMALL_MODEL: ${{ vars.GENAISCRIPT_DEFAULT_REASONING_SMALL_MODEL }}
    GENAISCRIPT_DEFAULT_MODEL: ${{ vars.GENAISCRIPT_DEFAULT_MODEL }}
    GENAISCRIPT_DEFAULT_SMALL_MODEL: ${{ vars.GENAISCRIPT_DEFAULT_SMALL_MODEL }}
    GENAISCRIPT_DEFAULT_VISION_MODEL: ${{ vars.GENAISCRIPT_DEFAULT_VISION_MODEL }}
jobs:
    tests:
        runs-on: ubuntu-latest
        steps:
            - uses: actions/checkout@v4
              with:
                  submodules: "recursive"
                  fetch-depth: 0
            - uses: actions/setup-node@v4
              with:
                  node-version: "20"
                  cache: yarn
            - run: yarn install --frozen-lockfile
            - name: compile
              run: yarn compile:action
            - name: install ffmpeg
              run: yarn ffmpeg:install
            - name: start ollama
              run: yarn ollama:start
            - name: start whisper
              run: yarn whisper:start
            - name: run summarize-ollama-phi3
              run: yarn test:summarize --model ollama:llama3.2:1b --out ./temp/summarize-ollama-phi3
              env:
                  OLLAMA_HOST: "http://localhost:11434"
            - name: run convert-ollama-phi3
              run: yarn cli convert summarize --model ollama:llama3.2:1b "packages/sample/src/rag/*.md" --cache-name sum
              env:
                  OLLAMA_HOST: "http://localhost:11434"
            - name: run transcribe
              run: yarn run:script video-transcript --model ollama:llama3.2:1b --out ./temp/summarize-ollama-phi3 --out-output $GITHUB_STEP_SUMMARY
              env:
                  OLLAMA_HOST: "http://localhost:11434"
