const urls = {
    openai: {
        url: "https://openai.com/api/pricing/",
        text: "Skip to main content\nResearch\nProducts\nSafety\nCompany\nPricing | OpenAI\nPricing\nShow prices per 1K tokens\nPricing\n\nSimple and flexible. Only pay for what you use.\n\nContact sales\nLatest models\n\nMultiple models, each with different capabilities and price points. Prices can be viewed in units of either per 1M or 1K tokens. You can think of tokens as pieces of words, where 1,000 tokens is about 750 words.\n\nLanguage models are also available in the Batch API\n(opens in a new window)\n that returns completions within 24 hours for a 50% discount.\n\nGPT-4o\n\nGPT-4o is our most advanced multimodal model that’s faster and cheaper than GPT-4 Turbo with stronger vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.\n\nLearn about GPT-4o\n(opens in a new window)\nModel\nPricing\nPricing with Batch API*\ngpt-4o\n$2.50 / 1M input tokens\n$1.25 / 1M input tokens\n$1.25 / 1M cached** input tokens\n$10.00 / 1M output tokens\n$5.00 / 1M output tokens\ngpt-4o-2024-08-06\n$2.50 / 1M input tokens\n$1.25 / 1M input tokens\n$1.25 / 1M cached** input tokens\n$10.00 / 1M output tokens\n$5.00 / 1M output tokens\ngpt-4o-2024-05-13\n$5.00 / 1M input tokens\n$2.50 / 1M input tokens\n$15.00 / 1M output tokens\n$7.50 / 1M output tokens\nVision pricing calculator\nSet model\ngpt-4o-2024-08-06\ngpt-4o-2024-05-13\ngpt-4o\nchatgpt-4o-latest\nSet width\npx\nby\nSet height\npx\n=\n$0.000638\nLow resolution\nPrice per 1M tokens (fixed)\t$2.50\n512 x 512 tiles\t1 × 1\nTotal tiles\t1\nBase tokens\t85\nTile tokens\t170 × 1 = 170\nTotal tokens\t255\nTotal price\t$0.000638\n\n*Batch API pricing requires requests to be submitted as a batch. Responses will be returned within 24 hours for a 50% discount. Learn more about Batch API ↗\n(opens in a new window)\n‎\n\n‎\n\n**Cached prompts are offered at a 50% discount compared to uncached prompts. Learn more about Prompt Caching ↗\n(opens in a new window)\n\n\n\nGPT-4o mini\n\nGPT-4o mini is our most cost-efficient small model that’s smarter and cheaper than GPT-3.5 Turbo, and has vision capabilities. The model has 128K context and an October 2023 knowledge cutoff.\n\nLearn about GPT-4o mini\n(opens in a new window)\nModel\nPricing\nPricing with Batch API*\ngpt-4o-mini\n$0.150 / 1M input tokens\n$0.075 / 1M input tokens\n$0.075 / 1M cached** input tokens\n$0.600 / 1M output tokens\n$0.300 / 1M output tokens\ngpt-4o-mini-2024-07-18\n$0.150 / 1M input tokens\n$0.075 / 1M input tokens\n$0.075 / 1M cached** input tokens\n$0.600 / 1M output tokens\n$0.300 / 1M output tokens\nVision pricing calculator\nSet width\npx\nby\nSet height\npx\n=\n$0.001275\nLow resolution\nPrice per 1M tokens (fixed)\t$0.15\n512 x 512 tiles\t1 × 1\nTotal tiles\t1\nBase tokens\t2833\nTile tokens\t5667 × 1 = 5667\nTotal tokens\t8500\nTotal price\t$0.001275\n\n*Batch API pricing requires requests to be submitted as a batch. Responses will be returned within 24 hours for a 50% discount. Learn more about Batch API ↗\n(opens in a new window)\n‎\n\n‎\n\n**Cached prompts are offered at a 50% discount compared to uncached prompts. Learn more about Prompt Caching ↗\n(opens in a new window)\n\n\n\nOpenAI o1-preview\n\no1-preview is our new reasoning model for complex tasks. The model has 128K context and an October 2023 knowledge cutoff.\n\nLearn about o1-preview\nModel\nPricing\no1-preview\n$15.00 / 1M input tokens\n$7.50 / 1M cached* input tokens\n$60.00 / 1M output** tokens\no1-preview-2024-09-12\n$15.00 / 1M input tokens\n$7.50 / 1M cached* input tokens\n$60.00 / 1M output** tokens\n\n*Output tokens include internal reasoning tokens generated by the model that are not visible in API responses.\n\nOpenAI o1-mini\n\no1-mini is a fast, cost-efficient reasoning model tailored to coding, math, and science use cases. The model has 128K context and an October 2023 knowledge cutoff.\n\nLearn about o1-mini\nModel\nPricing\no1-mini\n$3.00 / 1M input tokens\n$1.50 / 1M cached* input tokens\n$12.00 / 1M output* tokens\no1-mini-2024-09-12\n$3.00 / 1M input tokens\n$1.50 / 1M cached* input tokens\n$12.00 / 1M output* tokens\n\n*Output tokens include internal reasoning tokens generated by the model that are not visible in API responses.\n\n\nEmbedding models\n\nBuild advanced search, clustering, topic modeling, and classification functionality with our embeddings offering.\n\nLearn about embeddings\n(opens in a new window)\nModel\nPricing\nPricing with Batch API*\ntext-embedding-3-small\n$0.020 / 1M tokens\n$0.010 / 1M tokens\ntext-embedding-3-large\n$0.130 / 1M tokens\n$0.065 / 1M tokens\nada v2\n$0.100 / 1M tokens\n$0.050 / 1M tokens\n\n*Batch API pricing requires requests to be submitted as a batch. Responses will be returned within 24 hours for a 50% discount. Learn more about Batch API ↗\n(opens in a new window)\n\nFine-tuning models\n\nCreate your own custom models by fine-tuning our base models with your training data. Once you fine-tune a model, you’ll be billed only for the tokens you use in requests to that model.\n\nLearn about fine-tuning\n(opens in a new window)\nModel\nPricing\nPricing with Batch API*\ngpt-4o-2024-08-06**\n$3.750 / 1M input tokens\n$1.875 / 1M input tokens\n$15.000 / 1M output tokens\n$7.500 / 1M output tokens\n$25.000 / 1M training tokens\ngpt-4o-mini-2024-07-18**\n$0.300 / 1M input tokens\n$0.150 / 1M input tokens\n$1.200 / 1M output tokens\n$0.600 / 1M output tokens\n$3.000 / 1M training tokens\ngpt-3.5-turbo\n$3.000 / 1M input tokens\n$1.500 / 1M input tokens\n$6.000 / 1M output tokens\n$3.000 / 1M output tokens\n$8.000 / 1M training tokens\ndavinci-002\n$12.000 / 1M input tokens\n$6.000 / 1M input tokens\n$12.000 / 1M output tokens\n$6.000 / 1M output tokens\n$6.000 / 1M training tokens\nbabbage-002\n$1.600 / 1M input tokens\n$0.800 / 1M input tokens\n$1.600 / 1M output tokens\n$0.800 / 1M output tokens\n$0.400 / 1M training tokens\n\n*Batch API pricing requires requests to be submitted as a batch. Responses will be returned within 24 hours for a 50% discount. Learn more about Batch API ↗\n(opens in a new window)\n\n‎\n\n**Fine-tuning for GPT-4o and GPT-4o mini is free up to a daily token limit through October 31, 2024. For GPT-4o, each qualifying org gets up to 1M complimentary training tokens daily and any overage will be charged at the normal rate of $25.00/1M tokens. For GPT-4o mini, each qualifying org gets up to 2M complimentary training tokens daily and any overage will be charged at the normal rate of $3.00/1M tokens.\n\n\nRealtime API\n\nThe Realtime API lets developers build low-latency, multimodal experiences, including speech-to-speech capabilities. Text and audio processed by the Realtime API are priced separately.\n\nLearn about Realtime API\n(opens in a new window)\nPricing\ngpt-4o-realtime-preview\nText\n$5.00 / 1M input tokens\n$20.00 / 1M output tokens\nAudio*\n$100.00 / 1M input tokens\n$200.00 / 1M output tokens\ngpt-4o-realtime-preview-2024-10-01\nText\n$5.00 / 1M input tokens\n$20.00 / 1M output tokens\nAudio*\n$100.00 / 1M input tokens\n$200.00 / 1M output tokens\n\n*Audio input costs approximately 6¢ per minute; Audio output costs approximately 24¢ per minute\n\nAssistants API\n\nThe Assistants API and its tools make it easy for developers to build AI assistants in their applications. The tokens used for the Assistant API are billed at the chosen language model's per-token input / output rates.\n\nLearn about Assistants API\n(opens in a new window)\n\nAdditionally, we charge the following fees for tool usage:\n\nTool\nInput\nCode Interpreter\n$0.03 / session\nFile Search\n$0.10 / GB of vector-storage per day (1 GB free)\n\nGB refers to binary gigabytes (also known as gibibyte), where 1 GB is 2^30 bytes.\n\nImage models\n\nBuild DALL·E directly into your apps to generate and edit novel images and art. DALL·E 3 is the highest quality model and DALL·E 2 is optimized for lower cost.\n\nLearn about image generation\n(opens in a new window)\nModel\nQuality\nResolution\nPrice\nDALL·E 3\nStandard\n1024×1024\n$0.040 / image\nStandard\n1024×1792, 1792×1024\n$0.080 / image\nDALL·E 3\nHD\n1024×1024\n$0.080 / image\nHD\n1024×1792, 1792×1024\n$0.120 / image\nDALL·E 2\n1024×1024\n$0.020 / image\n512×512\n$0.018 / image\n256×256\n$0.016 / image\nAudio models\n\nWhisper can transcribe speech into text and translate many languages into English.\n\nText-to-speech (TTS) can convert text into spoken audio.\n\nLearn about Whisper\n(opens in a new window)\nLearn about Text-to-speech (TTS) \n(opens in a new window)\nModel\nUsage\nWhisper\n$0.006 / minute (rounded to the nearest second)\nTTS\n$15.000 / 1M characters\nTTS HD\n$30.000 / 1M characters\nOther Models\n\nWhile we continuously improve our latest models, here is a list of other models that we support.\n\nModel\nInput\nOutput\nchatgpt-4o-latest\n$5.00 / 1M tokens\n$15.00 / 1M tokens\ngpt-4-turbo\n$10.00 / 1M tokens\n$30.00 / 1M tokens\ngpt-4-turbo-2024-04-09\n$10.00 / 1M tokens\n$30.00 / 1M tokens\ngpt-4\n$30.00 / 1M tokens\n$60.00 / 1M tokens\ngpt-4-32k\n$60.00 / 1M tokens\n$120.00 / 1M tokens\ngpt-4-0125-preview\n$10.00 / 1M tokens\n$30.00 / 1M tokens\ngpt-4-1106-preview\n$10.00 / 1M tokens\n$30.00 / 1M tokens\ngpt-4-vision-preview\n$10.00 / 1M tokens\n$30.00 / 1M tokens\ngpt-3.5-turbo-0125\n$0.50 / 1M tokens\n$1.50 / 1M tokens\ngpt-3.5-turbo-instruct\n$1.50 / 1M tokens\n$2.00 / 1M tokens\ngpt-3.5-turbo-1106\n$1.00 / 1M tokens\n$2.00 / 1M tokens\ngpt-3.5-turbo-0613\n$1.50 / 1M tokens\n$2.00 / 1M tokens\ngpt-3.5-turbo-16k-0613\n$3.00 / 1M tokens\n$4.00 / 1M tokens\ngpt-3.5-turbo-0301\n$1.50 / 1M tokens\n$2.00 / 1M tokens\ndavinci-002\n$2.00 / 1M tokens\n$2.00 / 1M tokens\nbabbage-002\n$0.40 / 1M tokens\n$0.40 / 1M tokens\nSave 50% with Batch API\nFAQ\nWhat’s a token?\n\nYou can think of tokens as pieces of words used for natural language processing. For English text, 1 token is approximately 4 characters or 0.75 words. As a point of reference, the collected works of Shakespeare are about 900,000 words or 1.2M tokens.\n\nTo learn more about how tokens work and estimate your usage…\n\nExperiment with our interactive Tokenizer tool\n(opens in a new window)\n.\n\nLog in to your account and enter text into the Playground. The counter in the footer will display how many tokens are in your text.\n\nWhich model should I use?\n\nWe generally recommend that developers use either GPT-4o or GPT-4o mini, depending on the complexity of your tasks. GPT-4o generally performs better on a wide range of tasks, while GPT-4o mini is fast and inexpensive for simpler tasks. We recommend experimenting with these models in Playground\n(opens in a new window)\n to investigate which models provide the best price performance trade-off for your usage.\n\nHow will I know how many tokens I’ve used each month?\n\nLog in to your account to view your usage tracking dashboard\n(opens in a new window)\n. This page will show you how many tokens you’ve used during the current and past billing cycles.\n\nHow can I manage my spending?\n\nYou can set a monthly budget in your billing settings\n(opens in a new window)\n, after which we’ll stop serving your requests. There may be a delay in enforcing the limit, and you are responsible for any overage incurred. You can also configure an email notification threshold to receive an email alert once you cross that threshold each month. We recommend checking your usage tracking dashboard\n(opens in a new window)\n regularly to monitor your spend.\n\nFor customers managing work with Projects, you can set and manage billing restrictions per project\n(opens in a new window)\n in the Dashboard.\n\nIs the ChatGPT API included in the ChatGPT Plus, Teams, or Enterprise subscription?\n\nNo, OpenAI APIs are billed separately from ChatGPT Plus, Teams, and Enterprise. The API has its own pricing, which can be found at openai.com/pricing. ChatGPT subscription pricing can be found at openai.com/chatgpt/pricing/.\n\nDoes Playground usage count against my quota?\n\nYes, we treat Playground usage the same as regular API usage.\n\nHow is pricing calculated for Completions?\n\nChat completion\n(opens in a new window)\n requests are billed based on the number of input tokens sent plus the number of tokens in the output(s) returned by the API.\n\nYour request may use up to num_tokens(input) + [max_tokens * max(n, best_of)] tokens, which will be billed at the per-engine rates outlined at the top of this page.\n\nIn the simplest case, if your prompt contains 1500 tokens and you request a single 500 token completion from the gpt-4o-2024-05-13 API, your request will use 2000 tokens and will cost [(1500 * 5.00) + (500 * 15.00)] / 1000000 = $0.015.\n\nYou can limit costs by reducing prompt length or maximum response length, limiting usage of best_of/n , adding appropriate stop sequences, or using engines with lower per-token costs.\n\nHow is pricing calculated for Fine-tuning?\n\nThere are two components to fine-tuning pricing: training and usage.\n\nWhen training a fine-tuned model, the total tokens used will be billed according to our training rates. Note that the number of training tokens depends on the number of tokens in your training dataset and your chosen number of training epochs\n(opens in a new window)\n. The default number of epochs is 4.\n\n(Tokens in your training file * Number of training epochs) = Total training tokens\n\n\nOnce you fine-tune a model, you’ll be billed only for the tokens you use. Requests sent to fine-tuned models are billed at our usage rates.\n\nWhich models support vision capabilities and how is pricing calculated?\n\nThe following models support vision:\n\ngpt-4o\n\ngpt-4o-2024-08-06\n\ngpt-4o-2024-05-13\n\ngpt-4o-mini\n\ngpt-4o-mini-2024-07-18\n\ngpt-4-2024-04-09\n\ngpt-4-turbo\n\ngpt-4-vision-preview\n\ngpt-4-1106-vision-preview\n\nImages are converted into tokens with the specific number of tokens depending on the model used.\n\nIs there an SLA on the various models?\n\nYou can visit our Status page to monitor service availability and view historical uptime. If your company or application has specific requirements, please contact our sales team.\n\nStart creating with OpenAI’s powerful models.\nGet started\nContact sales\nOur research\nOverview\nIndex\nLatest advancements\nOpenAI o1\nGPT-4\nGPT-4o mini\nDALL·E 3\nSora\nChatGPT\nFor Everyone\nFor Teams\nFor Enterprises\nChatGPT login\n(opens in a new window)\nDownload\nAPI\nPlatform overview\nPricing\nDocumentation\n(opens in a new window)\nAPI login\n(opens in a new window)\nExplore more\nOpenAI for business\nStories\nSafety overview\nSafety overview\nCompany\nAbout us\nNews\nOur Charter\nSecurity\nResidency\nCareers\nTerms & policies\nTerms of use\nPrivacy policy\nBrand guidelines\nOther policies\nOpenAI © 2015–2024\nManage Cookies\n(opens in a new window)\n(opens in a new window)\n(opens in a new window)\n(opens in a new window)\n(opens in a new window)\n(opens in a new window)\n(opens in a new window)",
    },
    azure: {
        text: "",
        url: "https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/?msockid=33924d01e79d60ae3abd59eee6276132",
    },
}
function delay(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms))
}
let rows = []
for (const [provider, { url, text }] of Object.entries(urls)) {
    console.log(url)
    let md = text
    if (!md) {
        const page = await host.browse(url, {})
        await delay(1000)
        const html = await page.content()
        const tables = await HTML.convertTablesToJSON(html, {
            useFirstRowForHeadings: true,
        })
        md = tables?.length
            ? YAML.stringify(tables)
            : await HTML.convertToMarkdown(html)
    }

    const res = await prompt`Extract the LLM model pricing (in $) from the page
    and export the result in CSV with model,price_per_million_input_tokens,price_per_million_output_tokens.
    
    - ignore regional pricing

    PRICING:
    ${md}`

    const csv = await parsers.CSV(res.fences[0]?.content ?? res.text)
    rows.push(...csv.map((row) => ({ provider, ...row })))
}

const norm = (p) => parseFloat(p.replace(/[$,]/g, ""))
// normalize
rows = rows.filter(({ model }) => !/regional/i.test(model))
for (const row of rows) {
    row.model = row.model.toLowerCase()
    row.price_per_million_input_tokens = norm(
        row.price_per_million_input_tokens
    )
    row.price_per_million_output_tokens = norm(
        row.price_per_million_output_tokens
    )
}
// turn rows into a map where key is provider:model and the value is the rest of the fields
const pricing = Object.fromEntries(
    rows.map(({ provider, model, ...rest }) => [`${provider}:${model}`, rest])
)

await workspace.writeText(
    `packages/core/src/pricing.json`,    
    JSON.stringify(pricing, null, 2)
)
