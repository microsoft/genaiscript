<SYSTEM>Reference CLI: full reference documentation for the command line interface and Node.JS runtime</SYSTEM>

# Commands

> List of all CLI commands

<!-- autogenerated, do not edit --> A full list of the CLI command and its respective help text. ## `configure` [Section titled “configure”](#configure) ```plaintext Usage: genaiscript configure [options] Interactive help to configure providers Options: -p, --provider <string> Preferred LLM provider aliases (choices: "openai", "azure", "azure_ai_inference", "azure_serverless", "azure_serverless_models", "github", "ollama", "windows_ai", "anthropic", "anthropic_bedrock", "google", "huggingface", "mistral", "alibaba", "deepseek", "transformers", "lmstudio", "jan", "llamafile", "sglang", "vllm", "litellm", "whisperasr", "echo") -h, --help display help for command ``` ## `run` [Section titled “run”](#run) ```plaintext Usage: genaiscript run [options] <script> [files...] Runs a GenAIScript against files. Options: -a, --accept <string> comma separated list of accepted file extensions -p, --provider <string> Preferred LLM provider aliases (choices: "openai", "azure", "azure_ai_inference", "azure_serverless", "azure_serverless_models", "github", "ollama", "windows_ai", "anthropic", "anthropic_bedrock", "google", "huggingface", "mistral", "alibaba", "deepseek", "transformers", "lmstudio", "jan", "llamafile", "sglang", "vllm", "litellm", "whisperasr", "echo") -m, --model <string> 'large' model alias (default) -sm, --small-model <string> 'small' alias model -vm, --vision-model <string> 'vision' alias model -em, --embeddings-model <string> 'embeddings' alias model -ma, --model-alias <nameid...> model alias as name=modelid -re, --reasoning-effort <string> Reasoning effort for o* models (choices: "high", "medium", "low") -lp, --logprobs enable reporting token probabilities -tlp, --top-logprobs <number> number of top logprobs (1 to 5) -ef, --excluded-files <string...> excluded files -igi, --ignore-git-ignore by default, files ignored by .gitignore are excluded. disables this mode -ft, --fallback-tools Enable prompt-based tools instead of builtin LLM tool calling builtin tool calls -o, --out <string> output folder. Extra markdown fields for output and trace will also be generated -rmo, --remove-out remove output folder if it exists -ot, --out-trace <string> output file for trace -oo, --out-output <string> output file for output -od, --out-data <string> output file for data (.jsonl/ndjson will be aggregated). JSON schema information and validation will be included if available. -oa, --out-annotations <string> output file for annotations (.csv will be rendered as csv, .jsonl/ndjson will be aggregated) -ocl, --out-changelog <string> output file for changelogs -pr, --pull-request <number> pull request identifier -prc, --pull-request-comment [string] create comment on a pull request with a unique id (defaults to script id) -prd, --pull-request-description [string] create comment on a pull request description with a unique id (defaults to script id) -prr, --pull-request-reviews create pull request reviews from annotations -tm, --teams-message Posts a message to the teams channel -j, --json emit full JSON response to output -y, --yaml emit full YAML response to output -fe, --fail-on-errors fails on detected annotation error -r, --retry <number> number of retries (default: "10") -rd, --retry-delay <number> minimum delay between retries (default: "1000") -md, --max-delay <number> maximum delay between retries (default: "10000") -l, --label <string> label for the run -t, --temperature <number> temperature for the run -tp, --top-p <number> top-p for the run -mt, --max-tokens <number> maximum completion tokens for the run -mdr, --max-data-repairs <number> maximum data repairs -mtc, --max-tool-calls <number> maximum tool calls for the run -tc, --tool-choice <string> tool choice for the run, 'none', 'auto', 'required', or a function name -se, --seed <number> seed for the run -c, --cache enable LLM result cache -cn, --cache-name <name> custom cache file name -cs, --csv-separator <string> csv separator (default: "\t") -ff, --fence-format <string> fence format (choices: "xml", "markdown", "none") -ae, --apply-edits apply file edits --vars <namevalue...> variables, as name=value, stored in env.vars. Use environment variables GENAISCRIPT_VAR_name=value to pass variable through the environment -rr, --run-retry <number> number of retries for the entire run --no-run-trace disable automatic trace generation --no-output-trace disable automatic output generation -h, --help display help for command ``` ## `runs` [Section titled “runs”](#runs) ```plaintext Usage: genaiscript runs [options] [command] Commands to open previous runs Options: -h, --help display help for command Commands: list [script] List all available run reports in workspace help [command] display help for command ``` ### `runs list` [Section titled “runs list”](#runs-list) ```plaintext Usage: genaiscript runs list [options] [script] List all available run reports in workspace Arguments: script Script id Options: -h, --help display help for command ``` ## `test` [Section titled “test”](#test) ```plaintext Usage: genaiscript test|eval [options] [command] Options: -h, --help display help for command Commands: run [options] [script...] Runs the tests for scripts list [options] List available tests in workspace view Launch test viewer help [command] display help for command ``` ### `test run` [Section titled “test run”](#test-run) ```plaintext Usage: genaiscript test run [options] [script...] Runs the tests for scripts Arguments: script Script ids. If not provided, all scripts are tested Options: --redteam run red team tests -p, --provider <string> Preferred LLM provider aliases (choices: "openai", "azure", "azure_ai_inference", "azure_serverless", "azure_serverless_models", "github", "ollama", "windows_ai", "anthropic", "anthropic_bedrock", "google", "huggingface", "mistral", "alibaba", "deepseek", "transformers", "lmstudio", "jan", "llamafile", "sglang", "vllm", "litellm", "whisperasr", "echo") -m, --model <string> 'large' model alias (default) -sm, --small-model <string> 'small' alias model -vm, --vision-model <string> 'vision' alias model -em, --embeddings-model <string> 'embeddings' alias model -ma, --model-alias <nameid...> model alias as name=modelid -re, --reasoning-effort <string> Reasoning effort for o* models (choices: "high", "medium", "low") --models <models...> models to test where mode is the key value pair list of m (model), s (small model), t (temperature), p (top-p) --max-concurrency <number> maximum concurrency (default: "1") -o, --out <folder> output folder -rmo, --remove-out remove output folder if it exists --cli <string> override path to the cli -td, --test-delay <string> delay between tests in seconds --cache enable LLM result cache -v, --verbose verbose output -pv, --promptfoo-version [version] promptfoo version, default is 0.112.7 -os, --out-summary <file> append output summary in file -g, --groups <groups...> groups to include or exclude. Use :! prefix to exclude --test-timeout <number> test timeout in seconds -h, --help display help for command ``` ### `test list` [Section titled “test list”](#test-list) ```plaintext Usage: genaiscript test list [options] List available tests in workspace Options: --redteam list red team tests -g, --groups <groups...> groups to include or exclude. Use :! prefix to exclude -h, --help display help for command ``` ### `test view` [Section titled “test view”](#test-view) ```plaintext Usage: genaiscript test view [options] Launch test viewer Options: -h, --help display help for command ``` ## `convert` [Section titled “convert”](#convert) ```plaintext Usage: genaiscript convert [options] <script> [files...] Converts file through a GenAIScript. Each file is processed separately through the GenAIScript and the LLM output is saved to a <filename>.genai.md (or custom suffix). Options: -s, --suffix <string> suffix for converted files -rw, --rewrite rewrite input file with output (overrides suffix) -cw, --cancel-word <string> cancel word which allows the LLM to notify to ignore output -ef, --excluded-files <string...> excluded files -igi, --ignore-git-ignore by default, files ignored by .gitignore are excluded. disables this mode -p, --provider <string> Preferred LLM provider aliases (choices: "openai", "azure", "azure_ai_inference", "azure_serverless", "azure_serverless_models", "github", "ollama", "windows_ai", "anthropic", "anthropic_bedrock", "google", "huggingface", "mistral", "alibaba", "deepseek", "transformers", "lmstudio", "jan", "llamafile", "sglang", "vllm", "litellm", "whisperasr", "echo") -m, --model <string> 'large' model alias (default) -sm, --small-model <string> 'small' alias model -vm, --vision-model <string> 'vision' alias model -em, --embeddings-model <string> 'embeddings' alias model -ma, --model-alias <nameid...> model alias as name=modelid -re, --reasoning-effort <string> Reasoning effort for o* models (choices: "high", "medium", "low") -ft, --fallback-tools Enable prompt-based tools instead of builtin LLM tool calling builtin tool calls -o, --out <string> output folder. Extra markdown fields for output and trace will also be generated --vars <namevalue...> variables, as name=value, stored in env.vars. Use environment variables GENAISCRIPT_VAR_name=value to pass variable through the environment -c, --cache enable LLM result cache -cn, --cache-name <name> custom cache file name -cc, --concurrency <number> number of concurrent conversions --no-run-trace disable automatic trace generation --no-output-trace disable automatic output generation -h, --help display help for command ``` ## `scripts` [Section titled “scripts”](#scripts) ```plaintext Usage: genaiscript scripts|script [options] [command] Utility tasks for scripts Options: -h, --help display help for command Commands: list [options] [script...] List all available scripts in workspace create [options] [name] Create a new script fix [options] Write TypeScript definition files in the script folder to enable type checking. compile [folders...] Compile all scripts in workspace model [options] [script] List model connection information for scripts help|info <script> Show help information for a script ``` ### `scripts list` [Section titled “scripts list”](#scripts-list) ```plaintext Usage: genaiscript scripts list [options] [script...] List all available scripts in workspace Arguments: script Script ids Options: --unlisted show unlisted scripts --json output in JSON format -g, --groups <groups...> groups to include or exclude. Use :! prefix to exclude -h, --help display help for command ``` ### `scripts create` [Section titled “scripts create”](#scripts-create) ```plaintext Usage: genaiscript scripts create [options] [name] Create a new script Arguments: name Name of the script Options: -t, --typescript Generate TypeScript file (.genai.mts) (default: true) -h, --help display help for command ``` ### `scripts fix` [Section titled “scripts fix”](#scripts-fix) ```plaintext Usage: genaiscript scripts fix [options] Write TypeScript definition files in the script folder to enable type checking. Options: -gci, --github-copilot-instructions Write GitHub Copilot custom instructions for better GenAIScript code generation --docs Download documentation --force Fix all folders, including built-in system scripts -h, --help display help for command ``` ### `scripts compile` [Section titled “scripts compile”](#scripts-compile) ```plaintext Usage: genaiscript scripts compile [options] [folders...] Compile all scripts in workspace Arguments: folders Pattern to match files Options: -h, --help display help for command ``` ### `scripts model` [Section titled “scripts model”](#scripts-model) ```plaintext Usage: genaiscript scripts model [options] [script] List model connection information for scripts Arguments: script Script id or file Options: -t, --token show token -h, --help display help for command ``` ### `scripts help` [Section titled “scripts help”](#scripts-help) ```plaintext Usage: genaiscript scripts help|info [options] <script> Show help information for a script Arguments: script Script id Options: -h, --help display help for command ``` ## `cache` [Section titled “cache”](#cache) ```plaintext Usage: genaiscript cache [options] [command] Cache management Options: -h, --help display help for command Commands: clear [name] Clear cache help [command] display help for command ``` ### `cache clear` [Section titled “cache clear”](#cache-clear) ```plaintext Usage: genaiscript cache clear [options] [name] Clear cache Arguments: name Name of the cache, tests Options: -h, --help display help for command ``` ## `video` [Section titled “video”](#video) ```plaintext Usage: genaiscript video [options] [command] Video tasks Options: -h, --help display help for command Commands: probe <file> Probes metadata from a video/audio file extract-audio [options] <file> Transcode video/audio file extract-frames [options] <file> Extract video frames help [command] display help for command ``` ### `video probe` [Section titled “video probe”](#video-probe) ```plaintext Usage: genaiscript video probe [options] <file> Probes metadata from a video/audio file Arguments: file Audio or video file to inspect Options: -h, --help display help for command ``` ### `video extract-audio` [Section titled “video extract-audio”](#video-extract-audio) ```plaintext Usage: genaiscript video extract-audio [options] <file> Transcode video/audio file Arguments: file Audio or video file to transcode Options: -t, --transcription Convert audio for speech-to-text -h, --help display help for command ``` ### `video extract-frames` [Section titled “video extract-frames”](#video-extract-frames) ```plaintext Usage: genaiscript video extract-frames [options] <file> Extract video frames Arguments: file Audio or video file to transcode Options: -k, --keyframes Extract only keyframes (intra frames) -st, --scene-threshold <number> Extract frames with a minimum threshold -c, --count <number> maximum number of frames to extract -s, --size <string> size of the output frames wxh -f, --format <string> Image file format -h, --help display help for command ``` ## `retrieval` [Section titled “retrieval”](#retrieval) ```plaintext Usage: genaiscript retrieval|retreival [options] [command] RAG support Options: -h, --help display help for command Commands: index [options] <name> <files...> Index files for vector search vector|search [options] <query> [files...] Search using vector embeddings similarity fuzz [options] <query> [files...] Search using string distance help [command] display help for command ``` ### `retrieval index` [Section titled “retrieval index”](#retrieval-index) ```plaintext Usage: genaiscript retrieval index [options] <name> <files...> Index files for vector search Options: -ef, --excluded-files <string...> excluded files -igi, --ignore-git-ignore by default, files ignored by .gitignore are excluded. disables this mode -em, --embeddings-model <string> 'embeddings' alias model --database <string> Type of database to use for indexing (choices: "local", "azure_ai_search") -h, --help display help for command ``` ### `retrieval vector` [Section titled “retrieval vector”](#retrieval-vector) ```plaintext Usage: genaiscript retrieval vector|search [options] <query> [files...] Search using vector embeddings similarity Options: -ef, --excluded-files <string...> excluded files -tk, --top-k <number> maximum number of results -ms, --min-score <number> minimum score -h, --help display help for command ``` ### `retrieval fuzz` [Section titled “retrieval fuzz”](#retrieval-fuzz) ```plaintext Usage: genaiscript retrieval fuzz [options] <query> [files...] Search using string distance Options: -ef, --excluded-files <string...> excluded files -tk, --top-k <number> maximum number of results -ms, --min-score <number> minimum score -h, --help display help for command ``` ## `serve` [Section titled “serve”](#serve) ```plaintext Usage: genaiscript serve [options] Start a GenAIScript local web server Options: --port <number> Specify the port number, default: 8003 -k, --api-key <string> API key to authenticate requests -n, --network Opens server on 0.0.0.0 to make it accessible on the network -c, --cors <string> Enable CORS and sets the allowed origin. Use '*' to allow any origin. --dispatch-progress Dispatch progress events to all clients --github-copilot-chat-client Allow github_copilot_chat provider to connect to connected Visual Studio Code --remote <string> Remote repository URL to serve --remote-branch <string> Branch to serve from the remote --remote-force Force pull from remote repository --remote-install Install dependencies from remote repository -p, --provider <string> Preferred LLM provider aliases (choices: "openai", "azure", "azure_ai_inference", "azure_serverless", "azure_serverless_models", "github", "ollama", "windows_ai", "anthropic", "anthropic_bedrock", "google", "huggingface", "mistral", "alibaba", "deepseek", "transformers", "lmstudio", "jan", "llamafile", "sglang", "vllm", "litellm", "whisperasr", "echo") -m, --model <string> 'large' model alias (default) -sm, --small-model <string> 'small' alias model -vm, --vision-model <string> 'vision' alias model -em, --embeddings-model <string> 'embeddings' alias model -ma, --model-alias <nameid...> model alias as name=modelid -re, --reasoning-effort <string> Reasoning effort for o* models (choices: "high", "medium", "low") -h, --help display help for command ``` ## `mcp` [Section titled “mcp”](#mcp) ```plaintext Usage: genaiscript mcp|mcps [options] Starts a Model Context Protocol server that exposes scripts as tools Options: --groups <string...> Filter script by groups --ids <string...> Filter script by ids --startup <string> Startup script id, executed after the server is started --remote <string> Remote repository URL to serve --remote-branch <string> Branch to serve from the remote --remote-force Force pull from remote repository --remote-install Install dependencies from remote repository -p, --provider <string> Preferred LLM provider aliases (choices: "openai", "azure", "azure_ai_inference", "azure_serverless", "azure_serverless_models", "github", "ollama", "windows_ai", "anthropic", "anthropic_bedrock", "google", "huggingface", "mistral", "alibaba", "deepseek", "transformers", "lmstudio", "jan", "llamafile", "sglang", "vllm", "litellm", "whisperasr", "echo") -m, --model <string> 'large' model alias (default) -sm, --small-model <string> 'small' alias model -vm, --vision-model <string> 'vision' alias model -em, --embeddings-model <string> 'embeddings' alias model -ma, --model-alias <nameid...> model alias as name=modelid -re, --reasoning-effort <string> Reasoning effort for o* models (choices: "high", "medium", "low") -h, --help display help for command ``` ## `webapi` [Section titled “webapi”](#webapi) ```plaintext Usage: genaiscript webapi [options] Starts an Web API server that exposes scripts as REST endpoints (OpenAPI 3.1 compatible) Options: -n, --network Opens server on 0.0.0.0 to make it accessible on the network --port <number> Specify the port number, default: 8003 -c, --cors <string> Enable CORS and sets the allowed origin. Use '*' to allow any origin. --route <string> Route prefix, like /api --groups <string...> Filter script by groups --ids <string...> Filter script by ids --startup <string> Startup script id, executed after the server is started --remote <string> Remote repository URL to serve --remote-branch <string> Branch to serve from the remote --remote-force Force pull from remote repository --remote-install Install dependencies from remote repository -p, --provider <string> Preferred LLM provider aliases (choices: "openai", "azure", "azure_ai_inference", "azure_serverless", "azure_serverless_models", "github", "ollama", "windows_ai", "anthropic", "anthropic_bedrock", "google", "huggingface", "mistral", "alibaba", "deepseek", "transformers", "lmstudio", "jan", "llamafile", "sglang", "vllm", "litellm", "whisperasr", "echo") -m, --model <string> 'large' model alias (default) -sm, --small-model <string> 'small' alias model -vm, --vision-model <string> 'vision' alias model -em, --embeddings-model <string> 'embeddings' alias model -ma, --model-alias <nameid...> model alias as name=modelid -re, --reasoning-effort <string> Reasoning effort for o* models (choices: "high", "medium", "low") -h, --help display help for command ``` ## `parse` [Section titled “parse”](#parse) ```plaintext Usage: genaiscript parse|parsers [options] [command] <file...> Parse various outputs Arguments: file input JSONL files Options: -h, --help display help for command Commands: data [options] <file> Convert CSV, YAML, TOML, INI, XLSX, XML, MD/X frontmatter or JSON data files into various formats fence <language> <file> Extracts a code fenced regions of the given type pdf [options] <file> Parse a PDF into text and images docx [options] <file> Parse a DOCX into texts html [options] <file_or_url> Parse an HTML file to text code <file> [query] Parse code using tree sitter and executes a query tokens [options] <files...> Count tokens in a set of files tokenize [options] <file> Tokenizes a piece of text and display the tokens (in hex format) jsonl2json Converts JSONL files to a JSON file prompty [options] <file...> Converts .prompty files to genaiscript jinja2 [options] <file> Renders Jinja2 or prompty template secrets <file...> Applies secret scanning and redaction to files markdown [options] <file> Chunks markdown files ``` ### `parse data` [Section titled “parse data”](#parse-data) ```plaintext Usage: genaiscript parse data [options] <file> Convert CSV, YAML, TOML, INI, XLSX, XML, MD/X frontmatter or JSON data files into various formats Options: -f, --format <string> output format (choices: "json", "json5", "yaml", "ini", "csv", "md") -h, --help display help for command ``` ### `parse fence` [Section titled “parse fence”](#parse-fence) ```plaintext Usage: genaiscript parse fence [options] <language> <file> Extracts a code fenced regions of the given type Options: -h, --help display help for command ``` ### `parse pdf` [Section titled “parse pdf”](#parse-pdf) ```plaintext Usage: genaiscript parse pdf [options] <file> Parse a PDF into text and images Options: -i, --images extract images -o, --out <string> output folder -h, --help display help for command ``` ### `parse docx` [Section titled “parse docx”](#parse-docx) ```plaintext Usage: genaiscript parse docx [options] <file> Parse a DOCX into texts Options: -f, --format <string> output format (choices: "markdown", "html", "text") -h, --help display help for command ``` ### `parse html` [Section titled “parse html”](#parse-html) ```plaintext Usage: genaiscript parse html [options] <file_or_url> Parse an HTML file to text Arguments: file_or_url HTML file or URL Options: -f, --format <string> output format (choices: "markdown", "text") -o, --out <string> output file -h, --help display help for command ``` ### `parse code` [Section titled “parse code”](#parse-code) ```plaintext Usage: genaiscript parse code [options] <file> [query] Parse code using tree sitter and executes a query Options: -h, --help display help for command ``` ### `parse tokens` [Section titled “parse tokens”](#parse-tokens) ```plaintext Usage: genaiscript parse tokens [options] <files...> Count tokens in a set of files Options: -ef, --excluded-files <string...> excluded files -h, --help display help for command ``` ### `parse tokenize` [Section titled “parse tokenize”](#parse-tokenize) ```plaintext Usage: genaiscript parse tokenize [options] <file> Tokenizes a piece of text and display the tokens (in hex format) Arguments: file file to tokenize Options: -m, --model <string> encoding model -h, --help display help for command ``` ### `parse jsonl2json` [Section titled “parse jsonl2json”](#parse-jsonl2json) ```plaintext Usage: genaiscript parse jsonl2json [options] Converts JSONL files to a JSON file Options: -h, --help display help for command ``` ### `parse prompty` [Section titled “parse prompty”](#parse-prompty) ```plaintext Usage: genaiscript parse prompty [options] <file...> Converts .prompty files to genaiscript Arguments: file input JSONL files Options: -o, --out <string> output folder -h, --help display help for command ``` ### `parse jinja2` [Section titled “parse jinja2”](#parse-jinja2) ```plaintext Usage: genaiscript parse jinja2 [options] <file> Renders Jinja2 or prompty template Arguments: file input Jinja2 or prompty template file Options: --vars <namevalue...> variables, as name=value passed to the template -h, --help display help for command ``` ### `parse secrets` [Section titled “parse secrets”](#parse-secrets) ```plaintext Usage: genaiscript parse secrets [options] <file...> Applies secret scanning and redaction to files Arguments: file input files Options: -h, --help display help for command ``` ### `parse markdown` [Section titled “parse markdown”](#parse-markdown) ```plaintext Usage: genaiscript parse markdown [options] <file> Chunks markdown files Arguments: file input markdown file Options: -m, --model <string> encoding model -mt, --max-tokens <number> maximum tokens per chunk -h, --help display help for command ``` ## `info` [Section titled “info”](#info) ```plaintext Usage: genaiscript info [options] [command] Utility tasks Options: -h, --help display help for command Commands: help Show help for all commands system Show system information env [options] [provider] Show .env information ``` ### `info help` [Section titled “info help”](#info-help) ```plaintext Usage: genaiscript info help [options] Show help for all commands Options: -h, --help display help for command ``` ### `info system` [Section titled “info system”](#info-system) ```plaintext Usage: genaiscript info system [options] Show system information Options: -h, --help display help for command ``` ### `info env` [Section titled “info env”](#info-env) ```plaintext Usage: genaiscript info env [options] [provider] Show .env information Options: -t, --token show token -e, --error show errors -m, --models show models if possible -h, --help display help for command ``` ## `models` [Section titled “models”](#models) ```plaintext Usage: genaiscript models [options] [command] Options: -h, --help display help for command Commands: list [options] [provider] List all available models alias Show model alias mapping help [command] display help for command ``` ### `models list` [Section titled “models list”](#models-list) ```plaintext Usage: genaiscript models list [options] [provider] List all available models Options: -f, --format <string> output format (choices: "json", "yaml") -h, --help display help for command ``` ### `models alias` [Section titled “models alias”](#models-alias) ```plaintext Usage: genaiscript models alias [options] Show model alias mapping Options: -h, --help display help for command ```

# Configure

> Configure and validate the LLM connections.

Interactive command to configure and validate the LLM connections. ```bash npx genaiscript configure ```

# Convert

> Learn how to apply a script to many files and extract the output.

Converts a set of files, separately, using a script. ```bash npx genaiscript convert <script> "<files...>" ``` where `<script>` is the id or file path of the tool to run, and `<files...>` is the name of the spec file to run it on. Unlike `run` which works on all files at once, `convert` processes each file individually. ## Files [Section titled “Files”](#files) `convert` takes one or more [glob](https://en.wikipedia.org/wiki/Glob_\(programming\)) patterns to match files in the workspace. ```bash npx genaiscript run <script> "**/*.md" "**/*.ts" ``` ### —excluded-files \<files…> [Section titled “—excluded-files \<files…>”](#excluded-files-files) Excludes the specified files from the file set. ```sh npx genaiscript convert <script> <files> --excluded-files <excluded-files...> ``` ### —exclude-git-ignore [Section titled “—exclude-git-ignore”](#exclude-git-ignore) Exclude files ignored by the `.gitignore` file at the workspace root. ```sh npx genaiscript convert <script> <files> --exclude-git-ignore ``` ## Output [Section titled “Output”](#output) The output of each file is saved to a new or existing file. You can control the logic to decide which part of the output to save where to save it. By default, a conversion result of a file `<filename>` is saved to a file `<filename>.genai.md`. ### —suffix \<suffix> [Section titled “—suffix \<suffix>”](#suffix-suffix) The `--suffix` option allows you to specify a suffix to append to the output file name. ```sh npx genaiscript convert <script> <files> --suffix .genai.txt ``` GenAIScript will “unfence” output in the markdown that match the suffix (after `.genai`) automatically. So if the LLM generates ````markdown ```txt :) ``` ```` The converted content in `<filename>.genai.txt` will be `:)`. ### —rewrite [Section titled “—rewrite”](#rewrite) This flag override `suffix` and tells GenAIScript to rewrite the original file with the converted content. ```sh npx genaiscript convert <script> <files> --rewrite ``` ### —cancel-word \<word> [Section titled “—cancel-word \<word>”](#cancel-word-word) Specify the “ignore output, nothing to see here” keyword using the `-cw` flag. ```sh npx genaiscript convert <script> <files> --cancel-word "<NO>" ``` ## Read more [Section titled “Read more”](#read-more) The full list of options is available in the [CLI reference](/genaiscript/reference/cli/commands#convert).

# Run

> Learn how to execute genai scripts on files with streaming output to stdout, including usage of glob patterns, environment variables, and output options.

Runs a script on files and streams the LLM output to stdout or a folder from the workspace root. ```bash npx genaiscript run <script> "<files...>" ``` where `<script>` is the id or file path of the tool to run, and `<files...>` is the name of the spec file to run it on. Files can also include [glob](https://en.wikipedia.org/wiki/Glob_\(programming\)) pattern. ```sh npx genaiscript run code-annotator "src/*.ts" ``` If multiple files are specified, all files are included in `env.files`. ```sh npx genaiscript run <script> "src/*.bicep" "src/*.ts" ``` ## Files [Section titled “Files”](#files) `run` takes one or more [glob](https://en.wikipedia.org/wiki/Glob_\(programming\)) patterns to match files in the workspace. ```bash npx genaiscript run <script> "**/*.md" "**/*.ts" ``` ### Resource resolutions [Section titled “Resource resolutions”](#resource-resolutions) GenAIScript will automatically handle and resolve specific URI patterns. * `file://` - local file * `https://github.com/<owner>/<repo>/blob/<branch>/<path>` - GitHub file * `https://github.com/<owner>/<repo>.git/<file glob>` - GitHub repository and file glob * `gist://id/<file glob>` - GitHub Gist and file glob * `https://gist.github.com/<owner>/<id>/<file glob>` - GitHub Gist and file glob * `git://<owner>/<repo>.git/<file glob>` - Git repository and file glob ### Piping [Section titled “Piping”](#piping) `run` takes the stdin content and converts it into the `stdin` file. The LLM output is sent to `stdout`, while the rest of the logging is sent to `stderr`. ```bash cat README.md | genaiscript run summarize > summary.md ``` ### —excluded-files \<files…> [Section titled “—excluded-files \<files…>”](#excluded-files-files) Excludes the specified files from the file set. ```sh npx genaiscript run <script> <files> --excluded-files <excluded-files...> ``` ### —exclude-git-ignore [Section titled “—exclude-git-ignore”](#exclude-git-ignore) Exclude files ignored by the `.gitignore` file at the workspace root. ```sh npx genaiscript run <script> <files> --exclude-git-ignore ``` ## Configuration [Section titled “Configuration”](#configuration) ### —model … [Section titled “—model …”](#model) Configure the default or `large` model alias. Use `echo` to do a dry run and return the messages instead of calling a LLM provider. ## —provider … [Section titled “—provider …”](#provider) Loads a set of model aliases for the given LLM provider. ### —vars name=value name2=value2 … [Section titled “—vars name=value name2=value2 …”](#vars-namevalue-name2value2) Populate values in the `env.vars` map that can be used when running the prompt. ## Output [Section titled “Output”](#output) ### —out \<file|directory> [Section titled “—out \<file|directory>”](#out-filedirectory) Saves the results in a JSON file, along with markdown files of the output and the trace. ```sh npx genaiscript run <script> <files> --out out/res.json ``` If `file` does not end with `.json`, the path is treated as a directory path. ```sh npx genaiscript run <script> <files> --out tmp ``` ### —json [Section titled “—json”](#json) Output the entire response as JSON to the stdout. ### —yaml [Section titled “—yaml”](#yaml) Output the entire response as YAML to the stdout. ### —out-trace \<file> [Section titled “—out-trace \<file>”](#out-trace-file) Save the markdown trace to the specified file. ```sh npx genaiscript run <script> <files> --out-trace &lt;file&gt; ``` In a GitHub Actions workflow, you can use this feature to save the trace as a step summary (`GITHUB_STEP_SUMMARY`): .github/workflows/genaiscript.yml ```yaml - name: Run GenAIScript tool on spec run: | genaiscript run <script> <files> --out-trace $GITHUB_STEP_SUMMARY ``` In Azure Dev Ops, you can use the [task.uploadSummary](https://learn.microsoft.com/en-us/azure/devops/pipelines/scripts/logging-commands?view=azure-devops\&tabs=bash#uploadsummary-add-some-markdown-content-to-the-build-summary) in your pipeline to upload the trace as a summary. genaiscript.pipeline.yml ```yaml - script: npx --yes genaiscript run poem --out-trace $(System.DefaultWorkingDirectory)/trace.md displayName: "Run GenAIScript tool" continueOnError: true - script: echo "##vso[task.uploadsummary]$(System.DefaultWorkingDirectory)/trace.md" displayName: "append readme to pipeline report" ``` ### —out-annotations \<file> [Section titled “—out-annotations \<file>”](#out-annotations-file) Emit annotations in the specified file as a JSON array, JSON Lines, [SARIF](https://sarifweb.azurewebsites.net/) or a CSV file if the file ends with `.csv`. ```sh npx genaiscript run <script> <files> --out-annotations diags.csv ``` Use JSON lines (`.jsonl`) to aggregate annotations from multiple runs in a single file. ```sh npx genaiscript run <script> <files> --out-annotations diags.jsonl ``` ### —out-data \<file> [Section titled “—out-data \<file>”](#out-data-file) Emits parsed data as JSON, YAML or JSONL. If a JSON schema is specified and availabe, the JSON validation result is also stored. ```sh npx genaiscript run <script> <files> --out-data data.jsonl ``` ### —out-changelogs \<file> [Section titled “—out-changelogs \<file>”](#out-changelogs-file) Emit changelogs in the specified file as text. ```sh npx genaiscript run <script> <files> --out-changelogs changelogs.txt ``` ## Pull Requests and Issues[]() [Section titled “Pull Requests and Issues ”](#pull-requests-and-issues) The CLI can update a pull request/issue description and comments when running in a GitHub Action or Azure DevOps pipeline. ### GitHub Action workflow configuration [Section titled “GitHub Action workflow configuration”](#github-action-workflow-configuration) Update your workflow configuration to include the following: * add the `pull-requests: write` permission to the workflow/step ```yaml permissions: pull-requests: write ``` * set the `GITHUB_TOKEN` secret in the `env` when running the cli ```yaml - run: npx --yes genaiscript run ... -prc --out-trace $GITHUB_STEP_SUMMARY env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} ... # LLM secrets ``` ### Azure DevOps configuration [Section titled “Azure DevOps configuration”](#azure-devops-configuration) * add `<your projectname> Build Service` in the **Collaborator** role to the repository * pass secrets to scripts, including `System.AccessToken` ```yaml - script: npx genaiscript run ... -prd env: SYSTEM_ACCESSTOKEN: $(System.AccessToken) ... # LLM secrets ``` ### —pull-request-description \[tag] [Section titled “—pull-request-description \[tag\]”](#pull-request-description-tag) When running within a GitHub Action or Azure DevOps pipeline on a pull request, the CLI inserts the LLM output in the description of the pull request ([example](https://github.com/microsoft/genaiscript/pull/564)) ```sh npx genaiscript run ... -prd ``` The `tag` parameter is a unique id used to differentiate description generate by different runs. Default is the script id. ### —pull-request-comment \[tag]; [Section titled “—pull-request-comment \[tag\];”](#pull-request-comment-tag) Upserts a comment on the pull request/issue with the LLM output ([example](https://github.com/microsoft/genaiscript/pull/564#issuecomment-2200474305)) ```sh npx genaiscript run ... -prc ``` The `tag` parameter is a unique id used to differentiate description generate by different runs. Default is the script id. ### —pull-request-reviews [Section titled “—pull-request-reviews”](#pull-request-reviews) Create pull request review comments from each [annotations](/genaiscript/reference/scripts/annotations) ([example](https://github.com/microsoft/genaiscript/pull/564#pullrequestreview-2151692644)). ```sh npx genaiscript run ... -prr ``` ## Read more [Section titled “Read more”](#read-more) The full list of options is available in the [CLI reference](/genaiscript/reference/cli/commands#run).

# Serve

> Launch local web server.

Launch a local web server that is used to run the playground or Visual Studio Code. Run from the workspace root: ```bash npx genaiscript serve ``` ## port [Section titled “port”](#port) The default port is `8003`. You can specify the port by setting the `--port` flag. ```bash npx genaiscript serve --port 8004 ``` ## API key [Section titled “API key”](#api-key) The API key is used to authenticate the requests to the server. You can specify an API key by setting the `--api-key` flag or the `GENAISCRIPT_API_KEY` environment variable. ```bash npx genaiscript serve --api-key my-api-key ``` or .env ```txt GENAISCRIPT_API_KEY=my-api-key ``` The API key can be set in the `Authorization` header of a request or in the URL query parameter `api-key` (`http://localhost:8003/#api-key=my-api-key`) ## CORS [Section titled “CORS”](#cors) You can enable [Cross Origin Shared Resource](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) by setting the `--cors` flag or setting the `GENAISCRIPT_CORS_ORIGIN` environment variable. ```bash npx genaiscript serve --cors contoso.com ``` ## Network [Section titled “Network”](#network) You can bind the server to `0.0.0.0` and make it accessible from the network by setting the `--network` flag. You need this flag to make the server accessible from a container. ```bash npx genaiscript serve --network ``` We highly recommend setting the API key when running the server on the network. ## Dockerized [Section titled “Dockerized”](#dockerized) To run a minimal docker image with the server, first create a docker image with genaiscript and any required tool. ```sh docker build -t genaiscript -<<EOF FROM node:alpine RUN apk add --no-cache git && npm install -g genaiscript EOF ``` This creates a `genaiscript` image locally that you can use to launch the server. ```sh docker run --env GITHUB_TOKEN --env-file .env --name genaiscript --rm -it --expose 8003 -p 8003:8003 -v ${PWD}:/workspace -w /workspace genaiscript genaiscript serve --network ``` then open `http://localhost:8003` in your browser. ## OpenAI API endpoints [Section titled “OpenAI API endpoints”](#openai-api-endpoints) The server implements various OpenAI API compatible endpoints. You can use the server as a proxy to the OpenAI API by setting the `--openai` flag. The routes can be used to provide a stable access to the configured LLMs to other tools like promptfoo. ```bash npx genaiscript serve --openai ``` This will enable the following routes: ### `/v1/chat/completions` [Section titled “/v1/chat/completions”](#v1chatcompletions) Mostly compatible with OpenAI’s chat completions API. The server will forward the requests to the OpenAI API and return the response. * `stream` is not supported. ### `/v1/models` [Section titled “/v1/models”](#v1models) Returns the list of models and aliases available in the server.

# Test

> Learn how to run tests for your scripts using GenAIScript CLI with support for multiple AI models.

Runs the tests in scripts using [promptfoo](https://www.promptfoo.dev/). ```bash npx genaiscript test "<scripts...>" ``` You can override which models to use in the tests using `--models`: ```bash npx genaiscript test "<scripts...>" --models openai:gpt-4 ollama:phi3 ``` ## result viewer [Section titled “result viewer”](#result-viewer) Run the `test view` command to launch the test result viewer: ```bash npx genaiscript test view ```

# Video

> Learn about various video-related command

Some of the [video processing capabilities](/genaiscript/reference/scripts/videos) are also available in the cli. ### `video probe` [Section titled “video probe”](#video-probe) Returns the result of `ffprobe` in the console. ```sh genaiscript video probe myvid.mp4 ``` ### `video extract-audio` [Section titled “video extract-audio”](#video-extract-audio) Extracts the audio to a smaller format, optimized for transcription. ```sh genaiscript video extract-audio myvid.mp4 ``` ### `video extract-frames` [Section titled “video extract-frames”](#video-extract-frames) Extracts screenshots from the video. You can specify timestamps in seconds or `h:mm:ss`, or a count of videos. ```sh genaiscript video extract-video myvid.mp4 ```

# Node.JS API

> Learn how to import and use the Node.JS API to run scripts in an isolated worker thread, including environment variable configuration and integration details for enhanced flexibility.

GenAIScript runs in a (slightly modified) Node.JS environment where additional globals have been added. This environment is configured by the [cli](/genaiscript/reference/cli). Therefore, in order to run a GenAIScript in a “vanialla” Node.JS process, you will need to the **Node.JS `run` API**. This API loads and executes a GenAIScript script in a separate worker thread. This page describes how to import and use the GenAIScript as an API in your Node.JS application. ## Configuration [Section titled “Configuration”](#configuration) Assuming you have have added the cli as a dependency in your project, you can import the [cli](/genaiscript/reference/api) as follows: * npm ```sh npm i -D genaiscript ``` * pnpm ```sh pnpm add -D genaiscript ``` * yarn ```sh yarn add -D genaiscript ``` The API can be imported using imports from **“genaiscript/api”**. ```js import { run } from "genaiscript/api" ``` The imported `api.mjs` wrapper is a tiny, zero dependency loader that spawns a [Node.JS worker thread](https://nodejs.org/api/worker_threads.html) to run GenAIScript. * No pollution of the globals * No side effects on the process ## `run` [Section titled “run”](#run) The `run` function wraps the [cli run](/genaiscript/reference/cli/run) command. ```js import { run } from "genaiscript/api" const results = await run("summarize", ["myfile.txt"]) ``` ### Environment variables [Section titled “Environment variables”](#environment-variables) You can set the environment variables for the GenAIScript process by passing an object as the `env` field in the options. By default, the worker will inherit `process.env`. ```js const results = await run("summarize", ["myfile.txt"], { env: { MY_ENV_VAR: "value", }, }) ```

# Overview

> Comprehensive guide to using the GenAIScript CLI for automating tasks with AI scripts in Node.js environments.

The GenAIScript CLI **`genaiscript`** runs GenAIScript scripts outside of Visual Studio and in your [automation](/genaiscript/getting-started/automating-scripts). * npm ```sh npx genaiscript ... ``` * pnpm ```sh pnpx genaiscript ... ``` * yarn ```sh yarn dlx genaiscript ... ``` ## Prerequisites [Section titled “Prerequisites”](#prerequisites) The CLI is a Node.JS package hosted on [npm](https://www.npmjs.com/package/genaiscript). * Install [Node.JS LTS](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) (Node.JS includes npm and npx). ## Installation [Section titled “Installation”](#installation) * Install locally as a `devDependency` in your project. - npm ```sh npm i -D genaiscript ``` - pnpm ```sh pnpm add -D genaiscript ``` - yarn ```sh yarn add -D genaiscript ``` * Install it globally. ```sh npm install -g genaiscript ``` * Check that your node version is at least 20.\_ and npm 10.\_ by running this command. ```sh node -v npx -v ``` ```text v20.11.1 10.5.0 ``` ## No Installation (`npx`) [Section titled “No Installation (npx)”](#no-installation-npx) > `npx` is installed with **Node.JS**. Using [npx](https://docs.npmjs.com/cli/v10/commands/npx), you can run the cli without any prior installation steps. *npx* will install the tool on demand. npx also takes care of tricky operating system issues where the tool is not found in the path. ```sh npx genaiscript ... ``` * Add `--yes` to skip the confirmation prompt, which is useful in a CI scenario. ```sh npx --yes genaiscript ... ``` * Specify the version range to avoid unexpected behavior with cached installations of the CLI using npx. ```sh npx --yes genaiscript@^1.16.0 ... ``` ## Helper scripts [Section titled “Helper scripts”](#helper-scripts) To make sure that the TypeScript definition files are written and updated, you can add the following scripts to your `package.json`. package.json ```json { "scripts": { "postinstall": "genaiscript scripts fix", "postupdate": "genaiscript scripts fix", "genaiscript": "genaiscript" } } ``` The `genaiscript` is also a shorthand script that makes it easier to invoke the CLI using `npm run`: ```sh npm run genaiscript ... ``` ### Working behind a Proxy [Section titled “Working behind a Proxy”](#working-behind-a-proxy) Some optional packages used by the CLI do not support an installation behind an HTTP proxy, which is very common in an enterprise setting. If your work environment requires going through a proxy, you should use `npm install --omit=optional` to have optional packages fail gracefully during the installation. If your work environment requires going through a proxy, you can set one of the following environment variables (`HTTP_PROXY`, `HTTPS_PROXY`, `http_proxy` or `https_proxy`) to have the CLI use a proxy, e.g. `HTTP_PROXY=http://proxy.acme.com:3128`. ## Configuration [Section titled “Configuration”](#configuration) The CLI will load the [secrets](/genaiscript/getting-started/configuration) from the environment variables or a `./.env` file. You can override the default `.env` file name by adding the `--env .env.local` file, over even import both. ```sh npx genaiscript run <script> --env .env .env.local ``` ## Create a new script [Section titled “Create a new script”](#create-a-new-script) Creates a new script file in the `genaisrc` folder. ```sh npx genaiscript scripts create <name> ``` ## Compile scripts [Section titled “Compile scripts”](#compile-scripts) Runs the TypeScript compiler to find errors in the scripts. ```sh npx genaiscript scripts compile ``` ## Run a script [Section titled “Run a script”](#run-a-script) [Run a script](/genaiscript/reference/cli/run) on file and streams the LLM output to stdout. **Run from the workspace root**. ```sh npx genaiscript run <script> [files...] ``` where `<script>` is the id or file path of the tool to run, and `[files...]` is the name of the spec file to run it on. The CLI also supports UNIX-style piping. ```sh cat README.md | genaiscript run summarize > summary.md ``` ### Listing model configuration [Section titled “Listing model configuration”](#listing-model-configuration) Run the `script model` command to list the available scripts and their model configuration. This can be useful to diagnose configuration issues in CI/CD environments. ```sh npx genaiscript scripts model [script] ``` where \[script] can be a script id or a file path. ## Using a the CLI as a Node.JS API [Section titled “Using a the CLI as a Node.JS API”](#using-a-the-cli-as-a-nodejs-api) The CLI can be imported and [used as an API in your Node.JS application](/genaiscript/reference/api). ## About mixing files and `--vars` [Section titled “About mixing files and --vars”](#about-mixing-files-and---vars) Both `files` and `--vars` are variable command-line arguments. That is, they will consume all the following entries until a new option starts. Therefore ordering is important when mixing them. It is best to place the files, then follow with the `--vars` option. ```sh genaiscript run <script> [files...] --vars key1=value1 key2=value2 ``` * [parsing ambiguity](https://github.com/tj/commander.js/blob/HEAD/docs/options-in-depth.md#parsing-ambiguity) ## Topics [Section titled “Topics”](#topics) [Run ](/genaiscript/reference/cli/run)Learn how to execute genai scripts on files with streaming output to stdout, including usage of glob patterns, environment variables, and output options. [Convert ](/genaiscript/reference/cli/convert)Learn how to apply a script to many files and extract the output. [Serve ](/genaiscript/reference/cli/serve)Launch local web server. [Video ](/genaiscript/reference/cli/video)Learn about various video-related command [Test ](/genaiscript/reference/cli/test)Learn how to run tests for your scripts using GenAIScript CLI with support for multiple AI models. [Configure ](/genaiscript/reference/cli/configure)Configure and validate the LLM connections. [Commands ](/genaiscript/reference/cli/commands)List of all CLI commands