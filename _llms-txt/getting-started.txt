<SYSTEM>Getting Started: Sequence of short tutorials on how to get started with GenAIScript</SYSTEM>

# Getting Started

> Start developing with the GenAIScript VS Code Extension to create AI scripts efficiently.

GenAIScript is a scripting language that integrates LLMs into the scripting process using a simplified JavaScript syntax. Supported by our VS Code GenAIScript extension, it allows users to create, debug, and automate LLM-based scripts. [Play](https://youtube.com/watch?v=ENunZe--7j0) ## Preamble [Section titled “Preamble”](#preamble) Before you start writing GenAIScripts, you will need to configure your environment to have access to a LLM. The [configuration](/genaiscript/getting-started/configuration) covers this topic in details as they are a lot of options to consider. ## Hello World [Section titled “Hello World”](#hello-world) A GenAIScript is a JavaScript program that builds an LLM which is then executed by the GenAIScript runtime. Let’s start with a simple script that tells the LLM to generate a poem. In typical use, GenAIScript files have the naming convention `<scriptname>.genai.mjs` and are stored in the `genaisrc` directory in a repository. Let’s call this script `poem.genai.mjs`. poem.genai.mjs ```js $`Write a poem in code.` ``` The `$...` syntax is [template literal](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals) that renders to a user message to the LLM prompt. In this example, it would be: ```txt Write a poem in code. ``` In practice, your script may also import [system scripts](/genaiscript/reference/scripts/system) (automatically or manually specified) that add more messages to the requests. So the final JSON payload sent to the LLM server might look more like this: ```js { ... messages: [ { role: "system", content: "You are helpful. ..." }, { role: "user", content: "Write a poem in code." } ] } ``` GenAIScripts can be executed from the [command line](/genaiscript/reference/cli) or run with a right-click context menu selection inside Visual Studio Code. Because a GenAIScript is just JavaScript, the execution of a script follows the normal JavaScript evaluation rules. Once the script is executed, the generated messages are sent to the LLM server, and the response is processed by the GenAIScript runtime. * npm ```sh npx genaiscript run poem ``` * pnpm ```sh pnpx genaiscript run poem ``` * yarn ```sh yarn dlx genaiscript run poem ``` Here is an example output for this prompt (shortened) that got returned by OpenAI gpt-4o. ````markdown ```python def poem(): # In the silence of code, ... # And thus, in syntax sublime, # We find the art of the rhyme. ``` ```` GenAIScript supports extracting structured data and files from the LLM output as we will see later. ## Variables [Section titled “Variables”](#variables) GenAIScripts support a way to declare [prompt variables](/genaiscript/reference/scripts/context), which allow to include content into the prompt and to refer to it later in the script. Let’s take a look at a `summarize` script that includes the content of a file and asks the LLM to summarize it. summarize.genai.mjs ```js def("FILE", workspace.readText("some/relative/markdown.txt")) $`Summarize FILE in one sentence.` ``` In this snippet, we use `workspace.readText` to read the content of a file (path relatie to workspace root) and we use `def` to include it in the prompt as a `prompt variable`. We then “referenced” this variable in the prompt. prompt ````markdown FILE: ```text file="some/relative/markdown.txt" What is Markdown? Markdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents. Created by John Gruber in 2004, Markdown is now one of the world’s most popular markup languages. ``` Summarize FILE in one sentence. ```` The `def` function supports many configuration flags to control how the content is included in the prompt. For example, you can insert line numbers or limit the number of tokens. ```js def("FILE", ..., { lineNumbers: true, maxTokens: 100 }) ``` ## Files parameters [Section titled “Files parameters”](#files-parameters) GenAIScript are meant to work on a file or set of files. When you run a script in Visual Studio Code on a file or a folder, those files are passed to the script using the `env.files` variable. You can use this `env.files` to replace hard-coded paths and make your scripts more resuable. summarize.genai.mjs ```js // summarize all files in the env.files array def("FILE", env.files) $`Summarize FILE in one sentence.` ``` And now apply it to a bunch of files * npm ```sh npx genaiscript run summarize "**/*.md" ``` * pnpm ```sh pnpx genaiscript run summarize "**/*.md" ``` * yarn ```sh yarn dlx genaiscript run summarize "**/*.md" ``` ## Processing outputs [Section titled “Processing outputs”](#processing-outputs) GenAIScript processes the outputs of the LLM and extracts files, diagnostics and code sections when possible. Let’s update the summarizer script to specify an output file pattern. summarize.genai.mjs ```js // summarize all files in the env.files array def("FILE", env.files) $`Summarize each FILE in one sentence. Save each generated summary to "<filename>.summary"` ``` Given this input, the model returns a string, which the GenAIScript runtime interprets based on what the prompt requested from the model: ````markdown File src/samples/markdown-small.txt.summary: ```text Markdown is a lightweight markup language created by John Gruber in 2004, known for adding formatting elements to plaintext text documents. ``` ```` Because the prompt requested that a file be written, the model has responded with content describing the contents of the file that should be created. In this case, the model has chosen to call that file `markdown-small.txt.summary`. Our GenAIScript library parses the LLM output, interprets it, and in this case will create the file. If the script is invoked in VS Code, the file creation is exposed to the user via a [Refactoring Preview](https://code.visualstudio.com/docs/editor/refactoring#_refactor-preview) or directly saved to the file system. Of course, things can get more complex - with functions, schemas, … -, but this is the basic flow of a GenAIScript script. If you’re looking for an exhaustive list of prompting techniques, checkout [the prompt report](https://learnprompting.org/). ## Using tools [Section titled “Using tools”](#using-tools) [Tools](/genaiscript/reference/scripts/tools) are a way to register JavaScript callbacks with the LLM, they can be used execute code, search the web, … or read files! Here is an example of a script that uses the [`fs_read_file`](/genaiscript/reference/scripts/system#systemfs_read_file) tool to read a file and summarize it: summarize.genai.mjs ```js script({ tools: "fs_read_file" }) $` - read the file markdown.md - summarize it in one sentence. - save output to markdown.md.txt ` ``` A possible trace looks like as follows. As you can see we are not using the `def` function anymore, we expect the LLM to issue a call to the `fs_read_file` tool to read the file `markdown.md` so that it receives the content of that file. Note that this approach is less deterministic than using `def` as the LLM might not call the tool. Moreover it uses more tokens as the LLM has to generate the code to call the tool. Nonetheless, it is a powerful way to interact with the LLM. ## Using agents [Section titled “Using agents”](#using-agents) You can add one more layer of indirection and use [agent\_fs](/genaiscript/reference/scripts/system#systemagent_fs), a file system [agent](/genaiscript/reference/scripts/agents), to read the file. The agent combines a call to an LLM, and a set of tools related to file system queries. summarize.genai.mjs ```js script({ tools: "agent_fs" }) $` - read the file src/rag/markdown.md - summarize it in one sentence. - save output to file markdown.md.txt (override existing) ` ``` ## Next steps [Section titled “Next steps”](#next-steps) While GenAIScripts can be written with any IDE and run from the command line, users of the [extension in Visual Studio Code](/genaiscript/getting-started/installation) greatly benefit from the additional support for writing, debugging, and executing GenAIScript provided. We strongly recommend starting by installing the extension.

# Automating scripts

> Learn how to automate your scripts using the GenAIScript CLI for efficient batch processing and integration into CI/CD pipelines.

Once you have a script that you are happy with, you can automate it using the [command line interface](/genaiscript/reference/cli). ## Running a script using the CLI [Section titled “Running a script using the CLI”](#running-a-script-using-the-cli) The basic usage of the CLI is to [run](/genaiscript/reference/cli/run/) a script with a tool name and a list of files. ```sh npx --yes genaiscript run <script_id> <...files> ``` where `<script_id>` is the name of the script (e.g. filename without `.genai.mjs`) and `<...files>` is a list of files to run the script on. The CLI will use the secrets in the `.env` file, populate `env.files` with `<...files>`, run the script and emit the output to the standard output. You can use the CLI to run your scripts in a CI/CD pipeline. The CLI will return a non-zero exit code if the script fails, which can be used to fail the pipeline. ### Apply Edits [Section titled “Apply Edits”](#apply-edits) Add the `--apply-edits` flag to the CLI to automatically write the file edits. ```sh npx --yes genaiscript run <script> <...files> --apply-edits ``` Caution An LLM may generate arbitrary code that can be harmful to your system. We recommend that you review the modified code before executing it. This could be done through a separate branch and a pull request. You can also use a [container](/genaiscript/reference/scripts/container) to run the script in a sandboxed environment. Refer to [Security and Trust](/genaiscript/reference/security-and-trust) for more discussion. ## GitHub Action [Section titled “GitHub Action”](#github-action) [GitHub Actions](https://docs.github.com/en/actions) is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. This section explains how to integrate your GenAIScript in GitHub Actions workflows and pull requests. [GitHub Models](https://github.com/marketplace/models) provide a integrated way to run LLM inference from a GitHub Action. ### Configure GitHub Models [Section titled “Configure GitHub Models”](#configure-github-models) To use GitHub Models, you must add the `models: read` permission to your workflow, pass the `GITHUB_TOKEN` secret to the cli, and configure the cli to use GitHub Models. This can be done by setting the LLM provider to `github` in cli. ```yaml permissions: models: read ... - run: npx --yes genaiscript run <script> <...files> --provider github env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} ``` ### Configure secrets and variables [Section titled “Configure secrets and variables”](#configure-secrets-and-variables) Configure the [secrets](https://docs.github.com/en/actions/security-guides/using-secrets-in-github-actions) and [variables](https://docs.github.com/en/actions/learn-github-actions/variables) on your repository or organization so that GenAIScript can connect to your LLM. The secrets and variables should match the `.env` file in your local environment. ### Running a script [Section titled “Running a script”](#running-a-script) Use the [cli](/genaiscript/reference/cli/run/) to run the script in a GitHub Action. * Make sure to pass the secrets and variables to the script to give access to the LLM. * use the `--out <path>` flag to store the results in a directory so that you can upload them as an artifact. ```yaml - run: npx --yes genaiscript run <script> <...files> --out results env: # variables OPENAI_API_TYPE: ${{ env.OPENAI_API_TYPE }} OPENAI_API_BASE: ${{ env.OPENAI_API_BASE }} # secret, redacted OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} ``` ### Add the trace to the action summary [Section titled “Add the trace to the action summary”](#add-the-trace-to-the-action-summary) Use the `out-trace` flag to output the trace to the summary file, `$GITHUB_STEP_SUMMARY` (see [example](https://github.com/microsoft/genaiscript/actions/runs/9370477073#summary-25797526178)). ```yaml - run: npx --yes genaiscript run ... --out-trace $GITHUB_STEP_SUMMARY ``` ### Diff [Section titled “Diff”](#diff) You can use `git.diff` to execute a [git](https://git-scm.com/) `diff` command to retrieve changes in the current branch. ```js const changes = await git.diff({ base: "main" }) def("GIT_DIFF", changes, { language: "diff", maxTokens: 20000, }) ``` Note that you’ll need to pull `origin/main` branch to make this command work in an action. ```yaml - run: git fetch origin && git pull origin main:main ``` ### Storing output in artifacts [Section titled “Storing output in artifacts”](#storing-output-in-artifacts) Ensure that the directory containing the results is uploaded as an artifact. You can review the trace by opening the `res.trace.md` file. in the zipped artifact. ```yaml - uses: actions/upload-artifact@v4 if: always() with: path: | genairesults/** ``` ### Azure OpenAI with a Service Principal [Section titled “Azure OpenAI with a Service Principal”](#azure-openai-with-a-service-principal) The official documentation of the [azure login action](https://github.com/Azure/login?tab=readme-ov-file#azure-login-action) contains detailed steps on configure Azure resource access from GitHub Actions. 1. Create a Service Principal following [connect from azure secret](https://learn.microsoft.com/en-us/azure/developer/github/connect-from-azure-secret#prerequisites) guide. 2. Assign any role to the service principal (e.g. **Reader**) in your Azure subscription. You need this to login. 3. Assign the role **Cognitive Services OpenAI User** to the service principal. You need this so that the service principal can access the OpenAI resource. 4. Configure the [AZURE\_CREDENTIALS](https://learn.microsoft.com/en-us/azure/developer/github/connect-from-azure-secret#create-a-github-secret-for-the-service-principal) secret in your GitHub repository with the service principal credentials. ```json { "clientId": "<Client ID>", "clientSecret": "<Client Secret>", "subscriptionId": "<Subscription ID>", "tenantId": "<Tenant ID>" } ``` 5. Configure the `AZURE_OPENAI_API_ENDPOINT` in your repository GitHub Action variables. 6. Add the following step in your workflow to your GitHub action to login to Azure. genai.yml ```yaml - name: Azure Login action uses: azure/login@v2 with: creds: ${{ secrets.AZURE_CREDENTIALS }} ``` 7. Update each step that invokes the [cli](/genaiscript/reference/cli) to include the `AZURE_OPENAI_API_ENDPOINT` variable. ```yaml - name: run genai script run: npx --yes genaiscript run ... env: AZURE_OPENAI_API_ENDPOINT: ${{ env.AZURE_OPENAI_API_ENDPOINT }} ``` ## GitHub Pull request [Section titled “GitHub Pull request”](#github-pull-request) If your GitHub Action is triggered by a [pull request event](https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#pull_request), you can use the following flags to add comments: description, conversation and reviews. In order to create comments, the workflow must have the `pull-requests: write` [permission](https://docs.github.com/en/actions/using-jobs/assigning-permissions-to-jobs) and the `GITHUB_TOKEN` secret must be passed to the script. ```yaml permissions: pull-requests: write ... - run: npx --yes genaiscript run ... env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} ... ``` ### Update Description [Section titled “Update Description”](#update-description) The `--pull-request-description` inserts the LLM output into the pull request section (see [example](https://github.com/microsoft/genaiscript/pull/504)). The command takes an optional string argument to uniquely identify this comment, it is used to update the comment (default is the script id). ```yaml - run: npx --yes genaiscript run --pull-request-description ``` If you want to run this script when the pull request is ready for review, use the [`ready_for_review`](https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#pull_request), `opened`, `reopened` events. ```yaml on: pull_request: types: [ready_for_review, opened, reopened] ``` ### Conversation comment [Section titled “Conversation comment”](#conversation-comment) The `--pull-request-comment` adds the LLM output as a comment to the pull request conversation (see [example](https://github.com/microsoft/genaiscript/pull/504#issuecomment-2145273728)). The optional argument is an identifier for the comment (default is the script id) so that only one comment appears for the id. ```yaml - run: npx --yes genaiscript run --pull-request-comment env: ... ``` ### Review comments [Section titled “Review comments”](#review-comments) Use the `--pull-request-reviews` to convert [annotations](/genaiscript/reference/scripts/annotations) as review comments **to the last commit** on the pull request (see [example](https://github.com/microsoft/genaiscript/pull/504#pullrequestreview-2093960791)). ```yaml - run: npx --yes genaiscript run --pull-request-reviews env: ... ``` GenAIScript will automatically try to ignore duplicate review comments and only create new ones. To collect the changes of the last commit in the pull request branch (see [cool blog post](https://www.kenmuse.com/blog/the-many-shas-of-a-github-pull-request/)), you can try this git command: ```js const { stdout: changes } = await host.exec("git", [ "diff", "HEAD^", "HEAD", "--", "**.ts", ]) ```

# Best Practices

> Suggestions for using GenAIScripts more effectively

## GenAIScript allows chatbot users to create reusable scripts [Section titled “GenAIScript allows chatbot users to create reusable scripts”](#genaiscript-allows-chatbot-users-to-create-reusable-scripts) If you have used an LLM-based chatbot, like ChatGPT, you are familiar with the kinds of things that LLMs can do that ordinary software (that doesn’t use LLMs) cannot. For example, LLMs can review a document, write poetry, and analyze images, just as a starting point (with the caveat that sometimes they make mistakes). GenAIScript allows you to write a prompt that is embedded in a JavaScript framework so that the prompt can be parameterized, tested, debugged, reused, and run from a command line. ## Given the model the context it needs from documents [Section titled “Given the model the context it needs from documents”](#given-the-model-the-context-it-needs-from-documents) GenAIScript allows users to add documents to their prompts. This allows the LLM to have more background information related to the task it is being asked to do. In a GenAIScript, the JavaScript [`def`](/genaiscript/reference/scripts/context) command gives the LLM the contents of a document and defines a name that can be used in the prompt to refer to that document. Standard document formats, like [pdf](/genaiscript/reference/scripts/pdf) and [docx](/genaiscript/reference/scripts/docx) are supported so you just have to name the files and our libraries will extract the text automatically. You can parameterize the input context further using [`env.files`](/genaiscript/reference/scripts/context). ## Focus a GenAIScript on having the LLM do 1 thing well [Section titled “Focus a GenAIScript on having the LLM do 1 thing well”](#focus-a-genaiscript-on-having-the-llm-do-1-thing-well) Say I wanted to use a GenAIScript to write a white paper. Instead of asking the model to write the whole paper as one prompt, I would divide the task into different parts: write the introduction, write the recommendations, write the conclusion, etc. By breaking down the problem into subproblems, you can debug the script to accomplish the specific task well and then move on. ## Use the output of 1 GenAIScript as input to another [Section titled “Use the output of 1 GenAIScript as input to another”](#use-the-output-of-1-genaiscript-as-input-to-another) Combining the two points above, you can create a collection of inter-related scripts that accomplish a more ambitious goal. Depending on your level of expertise, the combination can be accomplished by using the command line interface to the scripts [CLI](/genaiscript/reference/cli) and using traditional software to connect them. ## Use the right LLM or other foundation model for the task [Section titled “Use the right LLM or other foundation model for the task”](#use-the-right-llm-or-other-foundation-model-for-the-task) There are currently many different choices of AI models. We outline how to connect many of these with GenAIScript in [configuration](/genaiscript/getting-started/configuration). They vary in capabilities and cost, with some being available as open source and usable (with the right GPU) for free. Consult the documentation for the specific LLM or other model you are using to understand how to write prompts that effectively communicate the task you want the AI to perform. Parameters between LLMs vary, for example, the size of the input context allowed, so make sure that the content you want to communicate to the LLM fits in its context window size.

# Configuration

> Set up your LLM connection and authorization with environment variables for seamless integration.

You will need to configure the LLM connection and authorization secrets. You can use remote (like OpenAI, Azure, etc.) and local models (like Ollama, Jan, LMStudio, etc.) with GenAIScript. ## Model selection [Section titled “Model selection”](#model-selection) The model used by the script is configured through the `model` field in the `script` function. The model name is formatted as `provider:model-name`, where `provider` is the LLM provider and the `model-name` is provider specific. ```js script({ model: "openai:gpt-4o", }) ``` ### Large, small, vision models [Section titled “Large, small, vision models”](#large-small-vision-models) You can also use the `small`, `large`, `vision` [model aliases](/genaiscript/reference/scripts/model-aliases) to use the default configured small, large and vision-enabled models. Large models are typically in the OpenAI gpt-4 reasoning range and can be used for more complex tasks. Small models are in the OpenAI gpt-4o-mini range, and are useful for quick and simple tasks. ```js script({ model: "small" }) ``` ```js script({ model: "large" }) ``` The model aliases can also be overridden from the [cli run command](/genaiscript/reference/cli/run), or environment variables or configuration file. [Learn more about model aliases](/genaiscript/reference/scripts/model-aliases). ```sh genaiscript run ... --model largemodelid --small-model smallmodelid ``` or by adding the `GENAISCRIPT_MODEL_LARGE` and `GENAISCRIPT_MODEL_SMALL` environment variables. .env ```txt GENAISCRIPT_MODEL_LARGE="azure_serverless:..." GENAISCRIPT_MODEL_SMALL="azure_serverless:..." GENAISCRIPT_MODEL_VISION="azure_serverless:..." ``` You can also configure the default aliases for a given LLM provider by using the `provider` argument. The default are documented in this page and printed to the console output. ```js script({ provider: "openai" }) ``` ```sh genaiscript run ... --provider openai ``` ### Model aliases [Section titled “Model aliases”](#model-aliases) In fact, you can define any alias for your model (only alphanumeric characters are allowed) through environment variables of the name `GENAISCRIPT_MODEL_ALIAS` where `ALIAS` is the alias you want to use. .env ```txt GENAISCRIPT_MODEL_TINY=... ``` Model aliases are always lowercased when used in the script. ```js script({ model: "tiny" }) ``` ## `.env` file and `.env.genaiscript` file [Section titled “.env file and .env.genaiscript file”](#env-file-and-envgenaiscript-file) GenAIScript uses a `.env` file (and `.env.genaiscript`) to load secrets and configuration information into the process environment variables. GenAIScript multiple `.env` files to load configuration information. 1. Create or update a `.gitignore` file in the root of your project and make it sure it includes `.env`. This ensures that you do not accidentally commit your secrets to your source control. .gitignore ```txt ... .env .env.genaiscript ``` 2. Create a `.env` file in the root of your project. * .gitignore * **.env** 3. Update the `.env` file with the configuration information (see below). Do Not Commit Secrets The `.env` file should never be commited to your source control! If the `.gitignore` file is properly configured, the `.env`, `.env.genaiscript` file will appear grayed out in Visual Studio Code. .gitignore ```txt ... .env ``` ### Custom .env file location [Section titled “Custom .env file location”](#custom-env-file-location) You can specify a custom `.env` file location through the CLI or an environment variable. * GenAIScript script loads the following `.env` files in order by default: * `~/.env.genaiscript` * `./.env.genaiscript` * `./.env` * by adding the `--env <...files>` argument to the CLI. Each `.env` file is imported in order and may override previous values. ```sh npx genaiscript ... --env .env .env.debug ``` * by setting the `GENAISCRIPT_ENV_FILE` environment variable. ```sh GENAISCRIPT_ENV_FILE=".env.local" npx genaiscript ... ``` * by specifying the `.env` file location in a [configuration file](/genaiscript/reference/configuration-files). \~/genaiscript.config.yaml ```json { "$schema": "https://microsoft.github.io/genaiscript/schemas/config.json", "envFile": [".env.local", ".env.another"] } ``` ### No .env file [Section titled “No .env file”](#no-env-file) If you do not want to use a `.env` file, make sure to populate the environment variables of the genaiscript process with the configuration values. Here are some common examples: * Using bash syntax ```sh OPENAI_API_KEY="value" npx --yes genaiscript run ... ``` * GitHub Action configuration .github/workflows/genaiscript.yml ```yaml run: npx --yes genaiscript run ... env: OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} ``` ## `configure` command [Section titled “configure command”](#configure-command) The [configure](/genaiscript/reference/cli/configure) command is an interactive command to configure and validate the LLM connections. ```sh npx genaiscript configure ``` ## OpenAI [Section titled “OpenAI”](#openai) `openai` is the OpenAI chat model provider. It uses the `OPENAI_API_...` environment variables. 1. [Upgrade your account](https://platform.openai.com/settings/organization/billing/overview) to get access to the models. You will get 404s if you do not have a paying account. 2. Create a new secret key from the [OpenAI API Keys portal](https://platform.openai.com/api-keys). 3. Update the `.env` file with the secret key. .env ```txt OPENAI_API_KEY=sk_... ``` 4. Find the model you want to use from the [OpenAI API Reference](https://platform.openai.com/docs/models/gpt-4o) or the [OpenAI Chat Playground](https://platform.openai.com/playground/chat). ![Screenshot of a user interface with a sidebar on the left and a dropdown menu titled 'Chat' with various options for AI models. The 'gpt-4o' option is highlighted and selected with a checkmark, described as 'High-intelligence flagship model for complex, multi-step tasks'. ](/genaiscript/_astro/openai-model-names.4qOpyXrM_Eb2fp.webp) 5. Set the `model` field in `script` to the model you want to use. ```js script({ model: "openai:gpt-4o", ... }) ``` ### Logging [Section titled “Logging”](#logging) You can enable the `genaiscript:openai` and `genaiscript:openai:msg` [logging namespaces](/genaiscript/reference/scripts/logging) for more information about the requests and responses: ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ---------------- | ---------------------- | | large | gpt-4.1 | | small | gpt-4.1-mini | | tiny | gpt-4.1-nano | | vision | gpt-4.1 | | vision\_small | gpt-4.1-mini | | embeddings | text-embedding-3-small | | reasoning | o1 | | reasoning\_small | o3-mini | | transcription | whisper-1 | | speech | tts-1 | | image | dall-e-3 | | intent | gpt-4.1-mini | ## GitHub Models[]() [Section titled “GitHub Models ”](#github-models) The [GitHub Models](https://github.com/marketplace/models) provider, `github`, allows running models through the GitHub Marketplace. This provider is useful for prototyping and subject to [rate limits](https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits) depending on your subscription. ```js script({ model: "github:openai/gpt-4o" }) ``` ### Codespace configuration [Section titled “Codespace configuration”](#codespace-configuration) If you are running from a [GitHub Codespace](https://github.com/features/codespaces), the token is already configured for you… It just works. ### GitHub Actions configuration [Section titled “GitHub Actions configuration”](#github-actions-configuration) As of [April 2025](https://github.blog/changelog/2025-04-14-github-actions-token-integration-now-generally-available-in-github-models/), you can use the GitHub Actions token (`GITHUB_TOKEN`) to call AI models directly inside your workflows. 1. Ensure that the `models` permission is enabled in your workflow configuration. genai.yml ```yaml permissions: models: read ``` 2. Pass the `GITHUB_TOKEN` when running `genaiscript` genai.yml ```yaml run: npx -y genaiscript run ... env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} ``` Read more in the [GitHub Documentation](https://docs.github.com/en/github-models/integrating-ai-models-into-your-development-workflow#using-ai-models-with-github-actions) ### Configuring with your own token [Section titled “Configuring with your own token”](#configuring-with-your-own-token) If you are not using GitHub Actions or Codespaces, you can use your own token to access the models. 1. Create a [GitHub personal access token](https://github.com/settings/tokens/new). The token should not have any scopes or permissions. 2. Update the `.env` file with the token. .env ```txt GITHUB_TOKEN=... ``` To configure a specific model, 1. Open the [GitHub Marketplace](https://github.com/marketplace/models) and find the model you want to use. 2. Copy the model name from the Javascript/Python samples ```js const modelName = "microsoft/Phi-3-mini-4k-instruct" ``` to configure your script. ```js script({ model: "github:microsoft/Phi-3-mini-4k-instruct", }) ``` If you are already using `GITHUB_TOKEN` variable in your script and need a different one for GitHub Models, you can use the `GITHUB_MODELS_TOKEN` variable instead. ### `o1-preview` and `o1-mini` models [Section titled “o1-preview and o1-mini models”](#o1-preview-and-o1-mini-models) Currently these models do not support streaming and system prompts. GenAIScript handles this internally. ```js script({ model: "github:openai/o1-mini", }) ``` ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ---------------- | ----------------------------- | | large | openai/gpt-4.1 | | small | openai/gpt-4.1-mini | | tiny | openai/gpt-4.1-nano | | vision | openai/gpt-4.1 | | embeddings | openai/text-embedding-3-small | | reasoning | openai/o3 | | reasoning\_small | openai/o3-mini | ### Limitations * Smaller context windows, and rate limiting * listModels * logprobs (and top logprobs) ignored * Ignore prediction of output tokens * topLogprobs ## Azure OpenAI[]() [Section titled “Azure OpenAI ”](#azure-openai) The [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions) provider, `azure` uses the `AZURE_OPENAI_...` environment variables. You can use a managed identity (recommended) or an API key to authenticate with the Azure OpenAI service. You can also use a service principal as documented in [automation](/genaiscript/getting-started/automating-scripts). ```js script({ model: "azure:deployment-id" }) ``` ### Managed Identity (Entra ID) [Section titled “Managed Identity (Entra ID)”](#managed-identity-entra-id) 1. Open your Azure OpenAI resource in the [Azure Portal](https://portal.azure.com) 2. Navigate to **Access Control (IAM)**, then **View My Access**. Make sure your user or service principal has the **Cognitive Services OpenAI User/Contributor** role. If you get a `401` error, click on **Add**, **Add role assignment** and add the **Cognitive Services OpenAI User** role to your user. 3. Navigate to **Resource Management**, then **Keys and Endpoint**. 4. Update the `.env` file with the endpoint. .env ```txt AZURE_OPENAI_API_ENDPOINT=https://....openai.azure.com ``` 5. Navigate to **deployments** and make sure that you have your LLM deployed and copy the `deployment-id`, you will need it in the script. 6. Open a terminal and **login** with [Azure CLI](https://learn.microsoft.com/en-us/javascript/api/overview/azure/identity-readme?view=azure-node-latest#authenticate-via-the-azure-cli). ```sh az login ``` 7. Update the `model` field in the `script` function to match the model deployment name in your Azure resource. ```js script({ model: "azure:deployment-id", ... }) ``` Set the `NODE_ENV` environment variable to `development` to enable the `DefaultAzureCredential` to work with the Azure CLI. Otherwise, it will use a chained token credential with `env`, `workload`, `managed identity`, `azure cli`, `azure dev cli`, `azure powershell`, `devicecode` credentials. ### Listing models [Section titled “Listing models”](#listing-models) There are two ways to list the models in your Azure OpenAI resource: use the Azure Management APIs or by calling into a custom `/models` endpoint. ### Using the management APIs (this is the common way) [Section titled “Using the management APIs (this is the common way)”](#using-the-management-apis-this-is-the-common-way) In order to allow GenAIScript to list deployments in your Azure OpenAI service, you need to provide the Subscription ID **and you need to use Microsoft Entra!**. 1. Open the Azure OpenAI resource in the [Azure Portal](https://portal.azure.com), open the **Overview** tab and copy the **Subscription ID**. 2. Update the `.env` file with the subscription id. .env ```txt AZURE_OPENAI_SUBSCRIPTION_ID="..." ``` 3. Test your configuration by running ```sh npx genaiscript models azure ``` #### Using the `/models` endpoint [Section titled “Using the /models endpoint”](#using-the-models-endpoint) This approach assumes you have set a OpenAI comptaible `/models` enpoint in your subscription that returns the list of deployments in a format compatible with the OpenAI API. You can set the `AZURE_OPENAI_API_MODELS_TYPE` environment variable to point to `openai`. .env ```txt AZURE_OPENAI_API_MODELS_TYPE="openai" ``` ### Custom credentials [Section titled “Custom credentials”](#custom-credentials) In some situations, the default credentials chain lookup may not work. In that case, you can specify an additional environment variable `AZURE_OPENAI_API_CREDENTIALS` with the type of credential that should be used. .env ```txt AZURE_OPENAI_API_CREDENTIALS=cli ``` The types are mapped directly to their [@azure/identity](https://www.npmjs.com/package/@azure/identity) credential types: * `cli` - `AzureCliCredential` * `env` - `EnvironmentCredential` * `powershell` - `AzurePowerShellCredential` * `devcli` - `AzureDeveloperCliCredential` * `workloadidentity` - `WorkloadIdentityCredential` * `managedidentity` - `ManagedIdentityCredential` Set `NODE_ENV` to `development` to use the `DefaultAzureCredential` with the GenAIScript. ### Custom token scopes [Section titled “Custom token scopes”](#custom-token-scopes) The default token scope for Azure OpenAI access is `https://cognitiveservices.azure.com/.default`. You can override this value using the `AZURE_OPENAI_TOKEN_SCOPES` environment variable. .env ```txt AZURE_OPENAI_TOKEN_SCOPES=... ``` ### API Version [Section titled “API Version”](#api-version) GenAIScript maintains a [default API version](https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation) to access Azure OpenAI. * current version: 2025-01-01-preview You can override this value using the `AZURE_OPENAI_API_VERSION` environment variable. .env ```txt AZURE_OPENAI_API_VERSION=2025-01-01-preview ``` You can also override the API version on a per-deployment basis by settings the `AZURE_OPENAI_API_VERSION_<deployment-id>` environment variable (where deployment-id is capitalized). .env ```txt AZURE_OPENAI_API_VERSION_GPT-4O=2025-01-01-preview ``` ### API Key [Section titled “API Key”](#api-key) 1. Open your [Azure OpenAI resource](https://portal.azure.com) and navigate to **Resource Management**, then **Keys and Endpoint**. 2. Update the `.env` file with the secret key (**Key 1** or **Key 2**) and the endpoint. .env ```txt AZURE_OPENAI_API_KEY=... AZURE_OPENAI_API_ENDPOINT=https://....openai.azure.com ``` 3. The rest of the steps are the same: Find the deployment name and use it in your script, `model: "azure:deployment-id"`. ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ----- | ---------------- | ### Limitations * Ignore prediction of output tokens ## Azure AI Foundry[]() [Section titled “Azure AI Foundry ”](#azure-ai-foundry) Azure AI Foundry provides access to serverless and deployed models, both for OpenAI and other providers. There are multiple ways to access those servers that are supported in GenAIScript: * without any deployment, using the [Azure AI Model Inference](#azure_ai_inference) provider, * with deployment for OpenAI models, using the [Azure AI OpenAI Serverless](#azure_serverless) provider, * with deployments for non-OpenAI models, use the [Azure AI Serverless Models](#azure_serverless_models) provider. You can deploy “serverless” models through [Azure AI Foundry](https://ai.azure.com/) and pay as you go per token. You can browse the [Azure AI Foundry model catalog](https://ai.azure.com/explore/models) and use the [serverless API](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-serverless-availability) filter to see the available models. There are two types of serverless deployments that require different configurations: OpenAI models and all other models. The OpenAI models, like `gpt-4o`, are deployed to `.openai.azure.com` endpoints, while the Azure AI models, like `Meta-Llama-3.1-405B-Instruct` are deployed to `.models.ai.azure.com` endpoints. They are configured slightly differently. ### Azure AI Inference[]() [Section titled “Azure AI Inference ”](#azure-ai-inference) The [Azure AI Model Inference API](https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/reference/reference-model-inference-api?tabs=javascript) provides a single endpoint to access a number of LLMs. This is a great way to experiment as you do not need to create deployments to access models. It supports both Entra ID and key-based authentication. ```js script({ model: "azure_ai_inference:gpt-4o" }) ``` [Play](https://youtube.com/watch?v=kh670Bxe_1E) #### Managed Identity (Entra ID) [Section titled “Managed Identity (Entra ID)”](#managed-identity-entra-id-1) 1. **Follow [these steps](https://learn.microsoft.com/en-us/azure/ai-foundry/model-inference/how-to/configure-entra-id?tabs=rest\&pivots=ai-foundry-portal) carefully** to configure the required Roles for your user. 2. Open <https://ai.azure.com/> and open your project 3. Configure the **Endpoint Target URL** as the `AZURE_AI_INFERENCE_API_ENDPOINT`. .env ```txt AZURE_AI_INFERENCE_API_ENDPOINT=https://<resource-name>.services.ai.azure.com/models ``` 4. Find the model name in the model catalog with the **Deployment options = Serverless API** filter and use it in your script, `model: "azure_id_inference:model-id"`. ```js script({ model: "azure_ai_inference:model-id" }) ``` #### API Key [Section titled “API Key”](#api-key-1) 1. Open <https://ai.azure.com/>, open your project and go the **Overview** page. 2. Configure the **Endpoint Target URL** as the `AZURE_AI_INFERENCE_API_ENDPOINT` variable and the key in `AZURE_AI_INFERENCE_API_KEY` in the `.env` file\***\*.\*\*** .env ```txt AZURE_AI_INFERENCE_API_ENDPOINT=https://<resourcename>.services.ai.azure.com/models AZURE_AI_INFERENCE_API_KEY=... ``` 3. Find the model name in the model catalog with the **Deployment options = Serverless API** filter and use it in your script, `model: "azure_id_inference:model-id"`. ```js script({ model: "azure_ai_inference:model-id" }) ``` #### API Version [Section titled “API Version”](#api-version-1) The default API version for Azure AI Inference is 2025-03-01-preview. You can change it by setting the `AZURE_AI_INFERENCE_API_VERSION` environment variable (see [Azure AI Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation)) .env ```txt AZURE_AI_INFERENCE_API_VERSION=2025-01-01-preview ``` ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ---------------- | ---------------------- | | large | gpt-4o | | small | gpt-4o-mini | | vision | gpt-4o | | vision\_small | gpt-4o-mini | | reasoning | o1 | | reasoning\_small | o1-mini | | embeddings | text-embedding-3-small | ### Limitations * listModels * logprobs (and top logprobs) ignored * Ignore prediction of output tokens * topLogprobs ### Azure AI OpenAI Serverless[]() [Section titled “Azure AI OpenAI Serverless ”](#azure-ai-openai-serverless) The `azure_serverless` provider supports OpenAI models deployed through the Azure AI Foundry serverless deployments. It supports both Entra ID and key-based authentication. ```js script({ model: "azure_serverless:deployment-id" }) ``` #### Managed Identity (Entra ID) [Section titled “Managed Identity (Entra ID)”](#managed-identity-entra-id-2) 1. Open <https://ai.azure.com/>, open your project and go the **Deployments** page. 2. Deploy a **base model** from the catalog. You can use the `Deployment Options` -> `Serverless API` option to deploy a model as a serverless API. 3. Deploy an OpenAI base model. This will also create a new Azure OpenAI resource in your subscription (which may be invisible to you, more later). 4. Update the `.env` file with the deployment endpoint in the `AZURE_SERVERLESS_OPENAI_API_ENDPOINT` variable. .env ```txt AZURE_SERVERLESS_OPENAI_API_ENDPOINT=https://....openai.azure.com ``` 5. Go back to the **Overview** tab in your Azure AI Foundry project and click on **Open in Management center**. 6. Click on the **Azure OpenAI Service** resource, then click on the **Resource** external link which will take you back to the (underlying) Azure OpenAI service in Azure Portal. 7. Navigate to **Access Control (IAM)**, then **View My Access**. Make sure your user or service principal has the **Cognitive Services OpenAI User/Contributor** role. If you get a `401` error, click on **Add**, **Add role assignment** and add the **Cognitive Services OpenAI User** role to your user. At this point, you are ready to login with the Azure CLI and use the managed identity. 1. Install the [Azure CLI](https://learn.microsoft.com/en-us/javascript/api/overview/azure/identity-readme?view=azure-node-latest#authenticate-via-the-azure-cli). 2. Open a terminal and login ```sh az login ``` #### API Key [Section titled “API Key”](#api-key-2) 1. Open your [Azure OpenAI resource](https://portal.azure.com) and navigate to **Resource Management**, then **Keys and Endpoint**. 2. Update the `.env` file with the endpoint and the secret key (**Key 1** or **Key 2**) and the endpoint. .env ```txt AZURE_SERVERLESS_OPENAI_API_ENDPOINT=https://....openai.azure.com AZURE_SERVERLESS_OPENAI_API_KEY=... ``` ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ---------------- | ---------------------- | | large | gpt-4o | | small | gpt-4o-mini | | vision | gpt-4o | | vision\_small | gpt-4o-mini | | reasoning | o1 | | reasoning\_small | o1-mini | | embeddings | text-embedding-3-small | ### Limitations * listModels * Ignore prediction of output tokens ### Azure AI Serverless Models[]() [Section titled “Azure AI Serverless Models ”](#azure-ai-serverless-models) The `azure_serverless_models` provider supports non-OpenAI models, such as DeepSeek R1/v3, deployed through the Azure AI Foundary serverless deployments. ```js script({ model: "azure_serverless_models:deployment-id" }) ``` #### Managed Identity (Entra ID) [Section titled “Managed Identity (Entra ID)”](#managed-identity-entra-id-3) 1. Open your **Azure AI Project** resource in the [Azure Portal](https://portal.azure.com) 2. Navigate to **Access Control (IAM)**, then **View My Access**. Make sure your user or service principal has the **Azure AI Developer** role. If you get a `401` error, click on **Add**, **Add role assignment** and add the **Azure AI Developer** role to your user. 3. Configure the **Endpoint Target URL** as the `AZURE_SERVERLESS_MODELS_API_ENDPOINT`. .env ```txt AZURE_SERVERLESS_MODELS_API_ENDPOINT=https://...models.ai.azure.com ``` 4. Navigate to **deployments** and make sure that you have your LLM deployed and copy the Deployment Info name, you will need it in the script. 5. Update the `model` field in the `script` function to match the model deployment name in your Azure resource. ```js script({ model: "azure_serverless:deployment-info-name", ... }) ``` #### API Key [Section titled “API Key”](#api-key-3) 1. Open <https://ai.azure.com/> and open the **Deployments** page. 2. Deploy a **base model** from the catalog. You can use the `Deployment Options` -> `Serverless API` option to deploy a model as a serverless API. 3. Configure the **Endpoint Target URL** as the `AZURE_SERVERLESS_MODELS_API_ENDPOINT` variable and the key in `AZURE_SERVERLESS_MODELS_API_KEY` in the `.env` file\***\*.\*\*** .env ```txt AZURE_SERVERLESS_MODELS_API_ENDPOINT=https://...models.ai.azure.com AZURE_SERVERLESS_MODELS_API_KEY=... ``` 4. Find the deployment name and use it in your script, `model: "azure_serverless_models:deployment-id"`. #### Support for multiple inference deployments [Section titled “Support for multiple inference deployments”](#support-for-multiple-inference-deployments) You can update the `AZURE_SERVERLESS_MODELS_API_KEY` with a list of `deploymentid=key` pairs to support multiple deployments (each deployment has a different key). .env ```txt AZURE_SERVERLESS_MODELS_API_KEY=" model1=key1 model2=key2 model3=key3 " ``` ### Limitations * listModels * Ignore prediction of output tokens ## Azure AI Search [Section titled “Azure AI Search”](#azure-ai-search) This is not a LLM provider, but a content search provider. However since it is configured similarly to the other Azure services, it is included here. It allows you to do [vector search](/genaiscript/reference/scripts/vector-search) of your documents using [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search). ```js const index = await retrieval.index("animals", { type: "azure_ai_search" }) await index.insertOrUpdate(env.files) const docs = await index.search("cat dog") ``` ### Managed Identity (Entra ID) [Section titled “Managed Identity (Entra ID)”](#managed-identity-entra-id-4) The service is configured through the `AZURE_AI_SEARCH_ENDPOINT` environment variable and the [configuration of the managed identity](https://learn.microsoft.com/en-us/azure/search/search-security-rbac?tabs=roles-portal-admin%2Croles-portal%2Croles-portal-query%2Ctest-portal%2Ccustom-role-portal). ```txt AZURE_AI_SEARCH_ENDPOINT=https://{{service-name}}.search.windows.net/ ``` 1. Open your **Azure AI Search** resource in the [Azure Portal](https://portal.azure.com), click on **Overview** and click on **Properties**. 2. Click on **API Access control** and enable **Role-based access control** or **Both**. 3. Open the **Access Control (IAM)** tab and make sure your user or service principal has the **Search Service Contributor** role. ### API Key [Section titled “API Key”](#api-key-4) The service is configured through the `AZURE_AI_SEARCH_ENDPOINT` and `AZURE_AI_SEARCH_API_KEY` environment variables. ```txt AZURE_AI_SEARCH_ENDPOINT=https://{{service-name}}.search.windows.net/ AZURE_AI_SEARCH_API_KEY=... ``` ## Google AI[]() [Section titled “Google AI ”](#google-ai) The `google` provider allows you to use Google AI models. It gives you access 1. Open [Google AI Studio](https://aistudio.google.com/app/apikey) and create a new API key. 2. Update the `.env` file with the API key. .env ```txt GEMINI_API_KEY=... ``` 3. Find the model identifier in the [Gemini documentation](https://ai.google.dev/gemini-api/docs/models/gemini) and use it in your script or cli with the `google` provider. ```py ... const model = genAI.getGenerativeModel({ model: "gemini-1.5-pro-latest", }); ... ``` then use the model identifier in your script. ```js script({ model: "google:gemini-1.5-pro-latest" }) ``` ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ---------------- | ---------------------------------- | | large | gemini-1.5-flash-latest | | small | gemini-1.5-flash-latest | | vision | gemini-1.5-flash-latest | | long | gemini-1.5-flash-latest | | reasoning | gemini-2.0-flash-thinking-exp-1219 | | reasoning\_small | gemini-2.0-flash-thinking-exp-1219 | | embeddings | text-embedding-004 | ### Limitations * Uses [OpenAI compatibility layer](https://ai.google.dev/gemini-api/docs/openai) * listModels * logprobs (and top logprobs) ignored * Ignore prediction of output tokens * Seed ignored * Tools implemented as fallback tools automatically. * topLogprobs ## GitHub Copilot Chat Models[]() [Section titled “GitHub Copilot Chat Models ”](#github-copilot-chat-models) If you have access to **GitHub Copilot Chat in Visual Studio Code**, GenAIScript will be able to leverage those [language models](https://code.visualstudio.com/api/extension-guides/language-model) as well. This mode is useful to run your scripts without having a separate LLM provider or local LLMs. However, those models are not available from the command line and have additional limitations and rate limiting defined by the GitHub Copilot platform. There is no configuration needed as long as you have GitHub Copilot installed and configured in Visual Studio Code. You can force using this model by using `github_copilot_chat:*` as a model name or set the **GenAIScript > Language Chat Models Provider** setting to true. This will default GenAIScript to use this provider for model aliases. [Play](https://youtube.com/watch?v=LRrVMiZgWJg) 1. Install [GitHub Copilot Chat](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat) (emphasis on **Chat**) 2. run your script 3. Confirm that you are allowing GenAIScript to use the GitHub Copilot Chat models. 4. select the best chat model that matches the one you have in your script ![A dropdown menu titled 'Pick a Language Chat Model for openai:gpt-4' with several options including 'GPT 3.5 Turbo', 'GPT 4', 'GPT 4 Turbo (2024-01-25 Preview)', and 'GPT 4o (2024-05-13)', with 'GPT 3.5 Turbo' currently highlighted. ](/genaiscript/_astro/vscode-language-models-select.B0vk7xsz_Z4UmRT.webp) (This step is skipped if you already have mappings in your settings) The mapping of GenAIScript model names to Visual Studio Models is stored in the settings. ## Anthropic [Section titled “Anthropic”](#anthropic) The `anthropic` provider access [Anthropic](https://www.anthropic.com/) models. Anthropic is an AI research company that offers powerful language models, including the Claude series. ```js script({ model: "anthropic:claude-2.1" }) ``` To use Anthropic models with GenAIScript, follow these steps: 1. Sign up for an Anthropic account and obtain an API key from their [console](https://console.anthropic.com/). 2. Add your Anthropic API key to the `.env` file: .env ```txt ANTHROPIC_API_KEY=sk-ant-api... ``` 3. Find the model that best suits your needs by visiting the [Anthropic model documentation](https://docs.anthropic.com/en/docs/about-claude/models#model-names). 4. Update your script to use the `model` you choose. ```js script({ ... model: "anthropic:claude-3-5-sonnet-20240620", }) ``` ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ---------------- | ----------------------------- | | large | claude-3-7-sonnet-latest | | small | claude-3-5-haiku-latest | | vision | claude-3-7-sonnet-latest | | vision\_small | claude-3-5-sonnet-latest | | reasoning | claude-3-7-sonnet-latest:high | | reasoning\_small | claude-3-7-sonnet-latest:low | ### Limitations * logprobs (and top logprobs) ignored * Ignore prediction of output tokens * topLogprobs ### Anthropic Bedrock[]() [Section titled “Anthropic Bedrock ”](#anthropic-bedrock) The `anthropic_bedrock` provider accesses Anthropic models on Amazon Bedrock. You can find the model names in the [Anthropic model documentation](https://docs.anthropic.com/en/docs/about-claude/models#model-names). GenAIScript assumes that you have configured AWS credentials in a way that the [AWS Node SDK will recognise](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/setting-credentials-node.html). ```js script({ model: "anthropic_bedrock:anthropic.claude-3-sonnet-20240229-v1:0" }) ``` ## Hugging Face[]() [Section titled “Hugging Face ”](#hugging-face) The `huggingface` provider allows you to use [Hugging Face Models](https://huggingface.co/models?other=text-generation-inference) using [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index). ```js script({ model: "huggingface:microsoft/Phi-3-mini-4k-instruct" }) ``` To use Hugging Face models with GenAIScript, follow these steps: 1. Sign up for a [Hugging Face account](https://huggingface.co/) and obtain an API key from their [console](https://huggingface.co/settings/tokens). If you are creating a **Fined Grained** token, enable the **Make calls to the serverless inference API** option. 2. Add your Hugging Face API key to the `.env` file as `HUGGINGFACE_API_KEY`, `HF_TOKEN` or `HUGGINGFACE_TOKEN` variables. .env ```txt HUGGINGFACE_API_KEY=hf_... ``` 3. Find the model that best suits your needs by visiting the [HuggingFace models](https://huggingface.co/models?other=text-generation-inference). 4. Update your script to use the `model` you choose. ```js script({ ... model: "huggingface:microsoft/Phi-3-mini-4k-instruct", }) ``` ### Logging [Section titled “Logging”](#logging-1) You can enable the `genaiscript:anthropic` and `genaiscript:anthropic:msg` [logging namespaces](/genaiscript/reference/scripts/logging) for more information about the requests and responses: ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ---------- | ---------------------------------------- | | large | meta-llama/Llama-3.3-70B-Instruct | | small | microsoft/phi-4 | | vision | meta-llama/Llama-3.2-11B-Vision-Instruct | | embeddings | nomic-ai/nomic-embed-text-v1.5 | ### Limitations * Uses [OpenAI compatibility layer](https://huggingface.github.io/text-generation-inference/) * listModels * Ignore prediction of output tokens ## Mistral AI[]() [Section titled “Mistral AI ”](#mistral-ai) The `mistral` provider allows you to use [Mistral AI Models](https://mistral.ai/technology/#models) using the [Mistral API](https://docs.mistral.ai/). ```js script({ model: "mistral:mistral-large-latest" }) ``` 1. Sign up for a [Mistral AI account](https://mistral.ai/) and obtain an API key from their [console](https://console.mistral.ai/). 2. Add your Mistral AI API key to the `.env` file: .env ```txt MISTRAL_API_KEY=... ``` 3. Update your script to use the `model` you choose. ```js script({ ... model: "mistral:mistral-large-latest", }) ``` ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ------ | -------------------- | | large | mistral-large-latest | | small | mistral-small-latest | | vision | pixtral-large-latest | ### Limitations * Ignore prediction of output tokens ## Alibaba Cloud[]() [Section titled “Alibaba Cloud ”](#alibaba-cloud) The `alibaba` provider access the [Alibaba Cloud](https://www.alibabacloud.com/) models. ```js script({ model: "alibaba:qwen-max", }) ``` 1. Sign up for a [Alibaba Cloud account](https://www.alibabacloud.com/help/en/model-studio/developer-reference/get-api-key) and obtain an API key from their [console](https://bailian.console.alibabacloud.com/). 2. Add your Alibaba API key to the `.env` file: .env ```txt ALIBABA_API_KEY=sk_... ``` 3. Find the model that best suits your needs by visiting the [Alibaba models](https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api). 4. Update your script to use the `model` you choose. ```js script({ ... model: "alibaba:qwen-max", }) ``` ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ---------- | ----------------- | | large | qwen-max | | small | qwen-turbo | | long | qwen-plus | | embeddings | text-embedding-v3 | ### Limitations * Uses [OpenAI compatibility layer](https://www.alibabacloud.com/help/en/model-studio/developer-reference/compatibility-of-openai-with-dashscope) * listModels * Ignore prediction of output tokens * Tools implemented as fallback tools automatically. ## Ollama [Section titled “Ollama”](#ollama) [Ollama](https://ollama.ai/) is a desktop application that lets you download and run models locally. Running tools locally may require additional GPU resources depending on the model you are using. Use the `ollama` provider to access Ollama models. 1. Start the Ollama application or ```sh ollama serve ``` 2. Update your script to use the `ollama:phi3.5` model (or any [other model](https://ollama.com/library) or from [Hugging Face](https://huggingface.co/docs/hub/en/ollama)). ```js script({ ..., model: "ollama:phi3.5", }) ``` GenAIScript will automatically pull the model, which may take some time depending on the model size. The model is cached locally by Ollama. 3. If Ollama runs on a server or a different computer or on a different port, you have to configure the `OLLAMA_HOST` environment variable to connect to a remote Ollama server. .env ```txt OLLAMA_HOST=https://<IP or domain>:<port>/ # server url OLLAMA_HOST=0.0.0.0:12345 # different port ``` You can specify the model size by adding the size to the model name, like `ollama:llama3.2:3b`. ```js script({ ..., model: "ollama:llama3.2:3b", }) ``` ### Ollama with Hugging Face models [Section titled “Ollama with Hugging Face models”](#ollama-with-hugging-face-models) You can also use [GGUF models](https://huggingface.co/models?library=gguf) from [Hugging Face](https://huggingface.co/docs/hub/en/ollama). ```js script({ ..., model: "ollama:hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF", }) ``` ### Ollama with Docker [Section titled “Ollama with Docker”](#ollama-with-docker) You can conviniately run Ollama in a Docker container. * if you are using a [devcontainer](https://code.visualstudio.com/devcontainers) or a [GitHub Codespace](https://github.com/features/codespaces), make sure to add the `docker-in-docker` option to your `devcontainer.json` file. ```json { "features": { "docker-in-docker": "latest" } } ``` * start the [Ollama container](https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image) ```sh docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama ``` * stop and remove the Ollama containers ```sh docker stop ollama && docker rm ollama ``` ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ---------- | ---------------- | | embeddings | nomic-embed-text | ### Limitations * Uses [OpenAI compatibility layer](https://github.com/ollama/ollama/blob/main/docs/openai.md) * logit\_bias ignored * Ignore prediction of output tokens ## DeepSeek [Section titled “DeepSeek”](#deepseek) `deepseek` is the [DeepSeek (https://www.deepseek.com/)](https://www.deepseek.com/) chat model provider. It uses the `DEEPSEEK_API_...` environment variables. 1. Create a new secret key from the [DeepSeek API Keys portal](https://platform.deepseek.com/usage). 2. Update the `.env` file with the secret key. .env ```txt DEEPSEEK_API_KEY=sk_... ``` 3. Set the `model` field in `script` to `deepseek:deepseek:deepseek-chat` which is currently the only supported model. ```js script({ model: "deepseek:deepseek-chat", ... }) ``` ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ------ | ---------------- | | large | deepseek-chat | | small | deepseek-chat | | vision | deepseek-chat | ## LM Studio[]() [Section titled “LM Studio ”](#lm-studio) The `lmstudio` provider connects to the [LMStudio](https://lmstudio.ai/) headless server. and allows to run local LLMs. 1. Install [LMStudio](https://lmstudio.ai/download) (v0.3.5+) 2. Open LMStudio 3. Open the [Model Catalog](https://lmstudio.ai/models), select your model and load it at least once so it is downloaded locally. 4. Open the settings (Gearwheel icon) and enable **Enable Local LLM Service**. 5. GenAIScript assumes the local server is at `http://localhost:1234/v1` by default. Add a `LMSTUDIO_API_BASE` environment variable to change the server URL. .env ```txt LMSTUDIO_API_BASE=http://localhost:2345/v1 ``` Find the model **API identifier** in the dialog of loaded models then use that identifier in your script: ```js script({ model: "lmstudio:llama-3.2-1b-instruct", }) ``` * GenAIScript uses the [LMStudio CLI](https://lmstudio.ai/docs/cli) to pull models on demand. * Specifiying the quantization is currently not supported. ### Aliases The following model aliases are attempted by default in GenAIScript. | Alias | Model identifier | | ---------- | ------------------------------------ | | embeddings | text-embedding-nomic-embed-text-v1.5 | ### Limitations * Ignore prediction of output tokens ### LM Studio and Hugging Face Models [Section titled “LM Studio and Hugging Face Models”](#lm-studio-and-hugging-face-models) Follow [this guide](https://huggingface.co/blog/yagilb/lms-hf) to load Hugging Face models into LMStudio. ## Jan [Section titled “Jan”](#jan) The `jan` provider connects to the [Jan](https://jan.ai/) local server. 1. [Jan](https://jan.ai/) 2. Open Jan and download the models you plan to use. You will find the model identifier in the model description page. 3. Click on the **Local API Server** icon (lower left), then **Start Server**. Keep the desktop application running! To use Jan models, use the `jan:modelid` syntax. If you change the default server URL, you can set the `JAN_API_BASE` environment variable. .env ```txt JAN_API_BASE=http://localhost:1234/v1 ``` ### Limitations * Ignore prediction of output tokens * top\_p ignored ## Windows AI [Section titled “Windows AI”](#windows-ai) The `window_ai` provider support [AI for Windows Apps](https://learn.microsoft.com/en-us/windows/ai/) which provides state-of-the-art local models, with NPU hardware support. 1. Install the [AI Toolkit for Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) extension. 2. Open the **Model Catalog** and add a model from the **ONNX Models** runtime section. 3. Right click on the model in the Explorer view and select **Copy model name** 4. Set the model name in your script to the model name you copied. ```js script({ model: "windows_ai:Phi-4-mini-gpu-int4-rtn-block-32", }) ``` See [Azure AI Toolkit getting started guide](https://learn.microsoft.com/en-us/windows/ai/toolkit/toolkit-getting-started). ## LocalAI [Section titled “LocalAI”](#localai) [LocalAI](https://localai.io/) act as a drop-in replacement REST API that’s compatible with OpenAI API specifications for local inferencing. It uses free Open Source models and it runs on CPUs. LocalAI acts as an OpenAI replacement, you can see the [model name mapping](https://localai.io/basics/container/#all-in-one-images) used in the container, like `gpt-4` is mapped to `phi-2`. 1. Install Docker. See the [LocalAI documentation](https://localai.io/basics/getting_started/#prerequisites) for more information. 2. Update the `.env` file and set the api type to `localai`. .env ```txt OPENAI_API_TYPE=localai ``` To start LocalAI in docker, run the following command: ```sh docker run -p 8080:8080 --name local-ai -ti localai/localai:latest-aio-cpu docker start local-ai docker stats echo "LocalAI is running at http://127.0.0.1:8080" ``` ## Llamafile [Section titled “Llamafile”](#llamafile) <https://llamafile.ai/> is a single file desktop application that allows you to run an LLM locally. The provider is `llamafile` and the model name is ignored. ## SGLang [Section titled “SGLang”](#sglang) [SGLang](https://docs.sglang.ai/) is a fast serving framework for large language models and vision language models. The provider is `sglang` and the model name is ignored. ## vLLM [Section titled “vLLM”](#vllm) [vLLM](https://docs.vllm.ai/) is a fast and easy-to-use library for LLM inference and serving. The provider is `vllm` and the model name is ignored. ## LLaMA.cpp [Section titled “LLaMA.cpp”](#llamacpp) [LLaMA.cpp](https://github.com/ggerganov/llama.cpp/tree/master/examples/server) also allow running models locally or interfacing with other LLM vendors. 1. Update the `.env` file with the local server information. .env ```txt OPENAI_API_BASE=http://localhost:... ``` ## OpenRouter [Section titled “OpenRouter”](#openrouter) You can configure the OpenAI provider to use the [OpenRouter](https://openrouter.ai/docs/quick-start) service instead by setting the `OPENAI_API_BASE` to `https://openrouter.ai/api/v1`. You will also need an [api key](https://openrouter.ai/settings/keys). .env ```txt OPENAI_API_BASE=https://openrouter.ai/api/v1 OPENAI_API_KEY=... ``` Then use the OpenRouter model name in your script: ```js script({ model: "openai:openai/gpt-4o-mini" }) ``` By default, GenAIScript will set the site URL and name to `GenAIScript` but you can override these settings with your own values: .env ```txt OPENROUTER_SITE_URL=... # populates HTTP-Referer header OPENROUTER_SITE_NAME=... # populate X-Title header ``` ## LiteLLM [Section titled “LiteLLM”](#litellm) The [LiteLLM](https://docs.litellm.ai/) proxy gateway provides a OpenAI compatible API for running models locally. Configure the `LITELLM_...` keys to set the key and optionally the base url. .env ```txt LITELLM_API_KEY="..." #LITELLM_API_BASE="..." ``` ## Hugging Face Transformer.js[]() [Section titled “Hugging Face Transformer.js ”](#hugging-face-transformerjs) Caution Temporarily disabled to reduce the installation footprint. This `transformers` provider runs models on device using [Hugging Face Transformers.js](https://huggingface.co/docs/transformers.js/index). The model syntax is `transformers:<repo>:<dtype>` where * `repo` is the model repository on Hugging Face, * [`dtype`](https://huggingface.co/docs/transformers.js/guides/dtypes) is the quantization type. ```js script({ model: "transformers:onnx-community/Qwen2.5-Coder-0.5B-Instruct:q4", }) ``` The default transformers device is `cpu`, but you can changing it using `HUGGINGFACE_TRANSFORMERS_DEVICE` environment variable. .env ```txt HUGGINGFACE_TRANSFORMERS_DEVICE=gpu ``` ### Limitations * Ignore prediction of output tokens ## Whisper ASR WebServices[]() [Section titled “Whisper ASR WebServices ”](#whisper-asr-webservices) This `whisperasr` provider allows to configure a [transcription](/genaiscript/reference/scripts/transcription) task to use the [Whisper ASR WebService project](https://ahmetoner.com/whisper-asr-webservice/). ```js const transcript = await transcribe("video.mp4", { model: "whisperasr:default", }) ``` This whisper service can run locally or in a docker container (see [documentation](https://ahmetoner.com/whisper-asr-webservice/)). CPU ```sh docker run -d -p 9000:9000 -e ASR_MODEL=base -e ASR_ENGINE=openai_whisper onerahmet/openai-whisper-asr-webservice:latest ``` You can also override the `transcription` model alias to change the default model used by `transcribe`. ## Echo [Section titled “Echo”](#echo) The `echo` provider is a dry run LLM provider that returns the messages without calling any LLM. It is most useful for debugging when you want to see the result LLM request without sending it. ```js script({ model: "echo", }) ``` Echo replies with the chat messages as markdown and JSON, which can be helpful for debugging. ## None [Section titled “None”](#none) The `none` provider prevents the execution of LLM. It is typically used on a top-level script that exclusively uses inline prompts. ```js script({ model: "none", }) ``` ## Custom Provider (OpenAI compatible) [Section titled “Custom Provider (OpenAI compatible)”](#custom-provider-openai-compatible) You can use a custom provider that is compatible with the [OpenAI text generation API](https://platform.openai.com/docs/guides/text-generation). This is useful for running LLMs on a local server or a different cloud provider. For example, to define a `ollizard` provider, you need to set the `OLLIARD_API_BASE` environment variable to the custom provider URL, and `OLLIZARD_API_KEY` if needed. .env ```txt OLLIZARD_API_BASE=http://localhost:1234/v1 #OLLIZARD_API_KEY=... ``` Then you can use this provider like any other provider. ```js script({ model: "ollizard:llama3.2:1b", }) ``` ## Model specific environment variables [Section titled “Model specific environment variables”](#model-specific-environment-variables) You can provide different environment variables for each named model by using the `PROVIDER_MODEL_API_...` prefix or `PROVIDER_API_...` prefix. The model name is capitalized and all non-alphanumeric characters are converted to `_`. This allows to have various sources of LLM computations for different models. For example, to enable the `ollama:phi3` model running locally, while keeping the default `openai` model connection information. .env ```txt OLLAMA_PHI3_API_BASE=http://localhost:11434/v1 ``` ## Running behind a proxy [Section titled “Running behind a proxy”](#running-behind-a-proxy) You can set the `HTTP_PROXY` and/or `HTTPS_PROXY` environment variables to run GenAIScript behind a proxy. .env ```txt HTTP_PROXY=http://proxy.example.com:8080 ``` ## Checking your configuration [Section titled “Checking your configuration”](#checking-your-configuration) You can check your configuration by running the `genaiscript info env` [command](/genaiscript/reference/cli). It will display the current configuration information parsed by GenAIScript. ```sh genaiscript info env ``` ## Next steps [Section titled “Next steps”](#next-steps) Write your [first script](/genaiscript/getting-started/your-first-genai-script).

# Debugging Scripts

> Learn how to debug GenAIScript files using Visual Studio Code Debugger to efficiently troubleshoot and enhance your JavaScript automation scripts.

The GenAIScript script files are executable JavaScript and can be debugged using the [Visual Studio Code Debugger](https://code.visualstudio.com/Docs/editor/debugging), just like any other JavaScript program. ![A screenshot of a debugging session in a code editor with a breakpoint set on a line of code. The editor is displaying several panels including the watch variables, call stack, and a terminal output. The code is partially visible with a function definition and JSON configuration data. ](/genaiscript/_astro/debugger.VhgOO6-1_ZMKDkn.webp) ## Starting a debugging session [Section titled “Starting a debugging session”](#starting-a-debugging-session) * Open the `.genai.mjs` file to debug and add breakpoints. ### From the env files [Section titled “From the env files”](#from-the-env-files) * Right click in the editor of the file you want in `env.files`. * Select the GenAIScript from the picker. #### From the script itself [Section titled “From the script itself”](#from-the-script-itself) * Add a `files` field in the `script` function ```js script({ ..., files: "*.md" }) ``` * Click on the **Debug** icon button on the editor menu (hidden under the run button). The debugger will launch the [cli](/genaiscript/reference/cli) and run the script in debug mode. The debugger will stop at the breakpoints you set. ## Limitations [Section titled “Limitations”](#limitations) The JavaScript executes in an external node process. Therefore, * The trace preview and output is not supported while debugging. ## Next steps [Section titled “Next steps”](#next-steps) Keep iterating the script or [add tests](/genaiscript/getting-started/testing-scripts).

# Installation

> Learn how to install GenAIScript as a Visual Studio Code extension or use it via command line for seamless integration into your development workflow.

GenAIScript is available as a [command line](#command-line) or a [Visual Studio Code Extension](#visual-studio-code-extension). ## Node.JS [Section titled “Node.JS”](#nodejs) GenAIScript requires [Node.JS](https://nodejs.org/) to run. We recommend installing the LTS version using a [node version manager](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm). 1. Install Node.JS (node) [with a package manager](https://nodejs.org/en/download/package-manager). **You need at least Node.JS v20.** 2. Check your installation ```sh node -v npx -v ``` You should see something similar or higher than the following versions: ```text v20.11.1 10.5.0 ``` ## Visual Studio Code Extension [Section titled “Visual Studio Code Extension”](#visual-studio-code-extension) The [Visual Studio Code Marketplace](https://marketplace.visualstudio.com/items?itemName=genaiscript.genaiscript-vscode) contains the latest stable release of the [extension](https://marketplace.visualstudio.com/items?itemName=genaiscript.genaiscript-vscode). 1. Install [Visual Studio Code](https://code.visualstudio.com/Download). Visual Studio Code is a lightweight but powerful source code editor which runs on your desktop and is available for Windows, macOS and Linux. 2. Open your project folder in Visual Studio Code. 3. Click on the **Extensions** view ![Icon representing the GenAIScript view in Visual Studio Code,&#x20; located in the activity bar on the left side of the screen. ](/genaiscript/_astro/vscode-extensions-view.wo0K8NqW_ZJhyag.webp) 4. Search **genaiscript** and click **Install**. ![Visual Studio Code Marketplace listing for 'GenAIScript' extension by Microsoft, featuring a logo with 'gen AI' in yellow on a black background, followed by the text 'GenAIScript Generative AI Scripting.' with a settings gear icon to the right. ](/genaiscript/_astro/vscode-marketplace.BBCYdcVx_Z1Ts97f.webp) 5. If successful, you will see the icon in the **Extensions** view. ![Icon for genAI script view in Visual Studio Code, featuring the text 'gen AI' in white on a dark background with a red outline. ](/genaiscript/_astro/vscode-genaiscript-view.CI7zhRNJ_2w1rKt.webp) 6. (Optional) Click on the ⚙️ gearwheel icon on the extension page and select **Add to Workspace Recommendations**. To install a specific version of the extension, we recommend storing the `genaiscript.vsix` in your repository and using the manual installation steps. ### Default Profile for Terminal [Section titled “Default Profile for Terminal”](#default-profile-for-terminal) GenAIScript launches a node server in the default terminal. If the default terminal is not configured or does not support node.js, you may need to update it in your user/workspace settings. * Open command palette `Ctrl+Shift+P` and search for `Terminal: Select Default Profile`. * Select the terminal profile like **Git Bash** ### Manual Installation (Advanced) [Section titled “Manual Installation (Advanced)”](#manual-installation-advanced) The latest development build of the extension is also available on through the GitHub releases. This allows access to bug fixes earlier than the marketplace release. 1. Open the [latest release](https://github.com/microsoft/genaiscript/releases/latest/) on GitHub 2. Download the `genaiscript.vsix` into your project root folder * … * **genaiscript.vsix** 3. Open your project in Visual Studio Code 4. Right click on the `.vsix` file and select **Install Extension VSIX…** ## Command Line [Section titled “Command Line”](#command-line) The [genaiscript](/genaiscript/reference/cli/) command line tool lets you run your GenAIScript from any terminal. ```sh npx genaiscript run my-script some/path/*.pdf ``` `npx` will automatically install and cache the CLI. ## DevContainer [Section titled “DevContainer”](#devcontainer) You can add this file in your project to use GenAIScript in a [DevContainer](https://containers.dev/), it contains a minimum of tools to get started.. .devcontainer/devcontainer.json ```json { "image": "mcr.microsoft.com/devcontainers/typescript-node:20", "customizations": { "vscode": { "extensions": ["genaiscript.genaiscript-vscode"] } } } ``` The devcontainer definition will automatically be used by [GitHub CodeSpaces](https://docs.github.com/en/codespaces/setting-up-your-project-for-codespaces/adding-a-dev-container-configuration/introduction-to-dev-containers), the [devcontainer cli](https://github.com/devcontainers/cli) or various [editor integrations](https://containers.dev/supporting). ## Next steps [Section titled “Next steps”](#next-steps) Let’s configure the [LLM connection information](/genaiscript/getting-started/configuration)

# Running scripts

> Discover how to run scripts in your development environment, manage script execution, and interpret the results for enhanced productivity.

Caution Script are executed in the context of your environment. **Only run trusted scripts.** ## Visual Studio Code [Section titled “Visual Studio Code”](#visual-studio-code) In Visual Studio Code, the location where you start running a script determines the entries in the [`env.files`](/genaiscript/reference/scripts/context) variable. [Play](https://youtube.com/watch?v=dM8blQZvvJg) ### Single file [Section titled “Single file”](#single-file) * Right click on a file in the Explorer and select **Run GenAIScript…**. * Or right click in a file editor and select **Run GenAIScript…**. The `env.files` array will contain a single element with the selected file. ![A file explorer window shows various files and folders. The file "Document.docx" is selected, and a context menu is open with the option "Run GenAIScript..." highlighted.](/genaiscript/_astro/vscode-file-run.D2SuwhFv_Z9M0xU.webp) ### Folder [Section titled “Folder”](#folder) * Right click on a folder in the Explorer and select \*\*Run GenAIScript…\*\*s. The `env.files` array will contain all nested files under that folder. ![The image shows a file explorer with a context menu. The "rag" folder is expanded, displaying files like "Document.docx." The context menu includes options like "New File," "Cut," "Copy," and "Run GenAIScript."](/genaiscript/_astro/vscode-folder-run.CqqhNtdL_sRgAa.webp) ### GitHub Copilot Chat [Section titled “GitHub Copilot Chat”](#github-copilot-chat) You can run scripts in the [GitHub Copilot Chat](https://code.visualstudio.com/docs/copilot/getting-started-chat) through the [**@genaiscript**](/genaiscript/reference/vscode/github-copilot-chat) participant. ![A screenshot of the chat participant window.](/genaiscript/_astro/chat-participant.BsdSg1Yh_u2VpW.webp) ### Default files [Section titled “Default files”](#default-files) You can specify default file or files to run the script on. When you run the script from the script file itself, or with the command line without file arguments, the default files will be used. ```js script({ files: "path/to/files*.md", }) ... ``` ### Tasks [Section titled “Tasks”](#tasks) The GenAIScript extension exposes each script as a [Task](https://code.visualstudio.com/docs/editor/tasks) automatically. The task launches the [cli](/genaiscript/reference/cli) and runs the selected script and pass the path to the current opened editor. * Open the command palette `Ctrl+Shift+P` and search “Tasks: Run Task” * Select the `genaiscript` task provider * Select the script you want to run ### Analyze results [Section titled “Analyze results”](#analyze-results) By default, GenAIScript opens the output preview which shows a rendered view of the LLM output (assuming the LLM produces markdown). The GenAIScript view provides an overview of the trace of the latest run. You can also use the **Trace** to review the each transformation step of the script execution. * Click on the GenAIScript status bar icon to various options to investigate results. ![A screenshot of a code editor shows a dropdown menu with "Retry," "Output," and "Trace" options, JSON data listing cities and populations, and a status bar indicating "150 tokens" generated by AI.](/genaiscript/_astro/vscode-statusbar-trace.Dnrt9G-1_8INE6.webp) ## Command Line [Section titled “Command Line”](#command-line) Start by creating a script using the [command line](/genaiscript/reference/cli). * JavaScript ```sh npx genaiscript scripts create proofreader ``` * TypeScript “—typescript” ```sh npx genaiscript scripts create proofreader --typescript ``` The `scripts create` command also drops a TypeScript definition file (`genaiscript.d.ts` and `tsconfig.json`) to enable type checking and auto-completion in your editor. If you need to regenerate the TypeScript definition file, use the `scripts fix` ```sh npx genaiscript scripts fix ``` Use the [run](/genaiscript/reference/cli/run) command to execute a script from the command line. ```sh npx genaiscript run proofreader path/to/files*.md ``` You can start a [playground](/genaiscript/reference/playground) to interactively run scripts through a similar web interface as the Visual Studio Code extension. ```sh npx genaiscript serve ``` ## Next steps [Section titled “Next steps”](#next-steps) [Debug](/genaiscript/getting-started/debugging-scripts) your scripts using the Visual Studio Code Debugger! ## Other integrations [Section titled “Other integrations”](#other-integrations) These are not actively maintained by the GenAIScript team, but we try to make them work as much as possible. If you find dragons, please report the issues. ### Cursor [Section titled “Cursor”](#cursor) GenAIScript can be installed in [Cursor](https://cursor.sh/how-to-install-extension) using the manual installation steps. ### [Neovim](https://neovim.io/) [Section titled “Neovim”](#neovim) The [genaiscript-runner.nvim](https://github.com/ryanramage/genaiscript-runner.nvim) project provides a plugin to run GenAIScript scripts.

# Testing scripts

> Learn how to declare and run tests for your scripts to ensure their correctness and reliability.

It is possible to declare [tests](/genaiscript/reference/scripts/tests) in the `script` function to validate the output of the script. ## Declaring tests [Section titled “Declaring tests”](#declaring-tests) The tests are added as an array of objects in the `tests` key of the `script` function. proofreader.genai.mjs ```js script({ ..., tests: { files: "src/rag/testcode.ts", rubrics: "is a report with a list of issues", facts: `The report says that the input string should be validated before use.`, } }) ``` ## Specifiying models [Section titled “Specifiying models”](#specifiying-models) You can also specify a set of models (and model aliases) to run the tests against. Each test will be run against each model. proofreader.genai.mjs ```js script({ ..., testModels: [ "azure_ai_inference:gpt-4o", "azure_ai_inference:gpt-4o-mini", "azure_ai_inference:deepseek-r1", ], }) ``` The `testModels` can be also overriden through the command line. ## Running tests [Section titled “Running tests”](#running-tests) ### Visual Studio Code [Section titled “Visual Studio Code”](#visual-studio-code) * Open the [Test Explorer view](https://code.visualstudio.com/docs/python/testing). * Select your script in the tree and click the `play` icon button. ![Visual Studio Test Explorer opened with a few genaiscript tests.](/genaiscript/_astro/vscode-test-explorer.DHobrdnh_1FDdux.webp) ### Command Line [Section titled “Command Line”](#command-line) Run this command from the workspace root. ```sh npx genaiscript test proofreader ``` ## Known limitations [Section titled “Known limitations”](#known-limitations) Currently, promptfoo treats the script source as the prompt text. Therefore, one cannot use assertions that also rely on the input text, such as `answer_relevance`. * Read more about [tests](/genaiscript/reference/scripts/tests) in the reference. ## Next steps [Section titled “Next steps”](#next-steps) [Automate](/genaiscript/getting-started/automating-scripts) script execution using the command line interface ([CLI](/genaiscript/reference/cli)).

# Tutorial Notebook

> Learn how to use GenAIScript with this interactive tutorial notebook featuring executable JavaScript code blocks.

This Notebook is a GenAIScript tutorial. It is a Markdown document where each JavaScript code section is a runnable GenAIScript. You can execute each code block individually and see the results in the output section below the code block. To open this notebook in Visual Studio Code, press **F1** and run **GenAIScript: Create GenAIScript Markdown Notebook**. Follow the steps in [configuration](https://microsoft.github.io/genaiscript/getting-started/configuration) to set up your environment and LLM access. ## Prompt as code [Section titled “Prompt as code”](#prompt-as-code) GenAIScript lets you write prompts as a JavaScript program. GenAIScript runs your program; generate chat messages; then handles the remaining interaction with the LLM API. ### Write to prompt `$` [Section titled “Write to prompt $”](#write-to-prompt) Let’s start with a simple hello world program. ```js $`Say "hello!" in emojis` ``` <!-- genaiscript output start --> <!-- genaiscript output end --> The `$` function formats the strings and write them to the user message. This user message is added to the chat messages and sent to the LLM API. Under the snippet, you can review both the **user** message (that our program generated) and the **assistant** (LLM) response. You can run the code block by clicking the **Execute Cell** button on the top left corner of the code block. It will be default try to use the LLMs from various providers. If you need to use a different model, update the `model` field in the front matter at the start of the document. There are many options documented in [configuration](https://microsoft.github.io/genaiscript/getting-started/configuration). Once the execution is done, you will also an additional **trace** entry that allows you to dive in the internal details of the GenAIScript execution. This is very helpful to diagnose issues with your prompts. The trace can be quite large so it is not serialized in the markdown file. You can use the JavaScript `for` loop and sequence multiple `$` calls to append text to the user message. You can also inner expression to generate dynamic content. ```js // let's give 3 tasks to the LLM // to get 3 different outputs for (let i = 1; i <= 3; i++) $`- Say "hello!" in ${i} emojis.` $`Respond with a markdown list` ``` <!-- genaiscript output start --> <!-- genaiscript output end --> To recap, the GenAIScript runs and generates a user messages; that gets sent to the LLM. You can review the user message (and others) in the trace. ## `def` and `env.files` [Section titled “def and env.files”](#def-and-envfiles) The [`def` function](https://microsoft.github.io/genaiscript/reference/scripts/context/#definition-def) lets you declare and assign **LLM variables**. The concept of variable is most useful to import context data, in particular files, and refer to them in the rest of the prompt. ```js def("FILE", env.files) $`Summarize FILE in one short sentence. Respond as plain text.` ``` <!-- genaiscript output start --> <!-- genaiscript output end --> In GenAIScript, the [`env.files`](https://microsoft.github.io/genaiscript/reference/scripts/context/#environment-env) variable contains the [list of files in context](https://microsoft.github.io/genaiscript/reference/script/files), which can be determined by a user selection in the UI, CLI arguments, or pre-configured like in this script. You can change the files in `env.files` by editing the `files` field in the front matter at the start of the document. ### Filtering `env.files` [Section titled “Filtering env.files”](#filtering-envfiles) When using GenAIScript from the user interface, it is common to apply a script to an entire folder. This means that you’ll get a bunch of files in `env.files` including some unneeded ones. The `def` function provides various options to filter the files, like the `endsWith` option. `def` also provides `maxTokens` which will trim the content size to a number of tokens. LLM context is finite! ```js script({ files: "src/**" }) // glob all files under src/samples def("FILE", env.files, { endsWith: ".md", maxTokens: 1000 }) // only consider markdown files $`Summarize FILE in one short sentence. Respond as plain text.` ``` <!-- genaiscript output start --> <!-- genaiscript output end --> ## Tools [Section titled “Tools”](#tools) You can register JavaScript functions as tools that the LLM will call as needed. ```js // requires openai, azure openai or github models defTool( "fetch", "Download text from a URL", { url: "https://..." }, ({ url }) => host.fetchText(url) ) $`Summarize https://raw.githubusercontent.com/microsoft/genaiscript/main/README.md in 1 sentence.` ``` ## Sub-prompt [Section titled “Sub-prompt”](#sub-prompt) You can run nested LLMs to execute tasks on other, smaller models. ```js // summarize each files individually for (const file of env.files) { const { text } = await runPrompt((_) => { _.def("FILE", file) _.$`Summarize the FILE.` }) def("FILE", { ...file, content: text }) } // summarize all summaries $`Summarize FILE.` ```

# Your first GenAI script

> Learn how to create and execute your initial GenAI script to automate interactions with language models.

GenAIScript use stylized JavaScript with minimal syntax. They are stored as JavaScript files (`genaisrc/*.genai.mjs)` or TypeScript files (`genaisrc/*.genai.mts`) in your project. The execution of a genaiscript creates the prompt that will be sent to the LLM. 1. * Visual Studio Code, Cursor Use the `> GenAIScript: Create new script...` command in the [command palette](https://code.visualstudio.com/docs/getstarted/userinterface#_command-palette) (`Ctrl+Shift+P` on Windows/Linux, `⇧⌘P` on Mac) to create a new script. ![A search bar with the text "createn" entered, displaying a suggestion for "GenAIScript: Create new script..." in a dropdown menu.](/genaiscript/_astro/vscode-create-new-script.Bia2CKYb_ZCe3CB.webp) * Other Editors Run the [cli](/genaiscript/reference/cli/) `script create` command with the name of the script you want to create. ```bash npx genaiscript script create proofreader ``` 2. The resulting file will be placed in the `genaisrc` folder in your project. * … * … ## the Prompt [Section titled “the Prompt”](#the-prompt) The execution of the GenAIScript generates a prompt (and more) that gets sent to the LLM model. The ` $``...`` ` template string function formats and write the string to the prompt; which gets sent to the LLM. poem.genai.mts ```js $`Write a one sentence poem.` ``` ## the Context [Section titled “the Context”](#the-context) GenAIScript exposes the context through the `env` variable. The context is implicitly defined by the location you start executing the script. * you can right click on a folder and the `env.files` will contain all the files nested in that folder. * you can right click on or in a file and the `env.files` will contain only that file. * you can run the script using the [command line interface](/genaiscript/reference/cli) and specify content of `env.files` in the CLI arguments. proofreader.genai.mts ```js def("FILES", env.files) ``` ## the Task [Section titled “the Task”](#the-task) The `$` function is used to build the prompt text, it renders and writes the text to the prompt (`$` is a [template literal](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals)). proofreader.genai.mts ```js def("FILES", env.files) $`You are an expert technical writer and proofreader. Review the documents in FILE and report the 2 most important issues.` ``` ## the Metadata [Section titled “the Metadata”](#the-metadata) You can add a call to the `script` function to provides metadata about the script and the model. The metadata is used to display the script in the UI and configure the LLM model. proofreader.genai.mts ```js // the metadata script({ // user interface title: "Technical proofreading", description: "Reviews the text as a tech writer.", group: "documentation", // model configuration model: "large", temperature: 0, }) def("FILES", env.files) $`You are an expert technical writer and proofreader. Review the documents in FILE and report the 2 most important issues.` ``` The `title`, `description`, and `group` properties are used to display the script in the UI and can be helpful when the user is searching for a script. ![A screenshot of a text editor showing a task labeled "Technical proofreading" with the description "Reviews the text as a tech writer." A hyperlink labeled "documentation" is on the right.](/genaiscript/_astro/vscode-select-script.CsuuFbnn_Z1O5H97.webp) ## Next steps [Section titled “Next steps”](#next-steps) * Follow the [Prompt As Code guide](/genaiscript/guides/prompt-as-code) to dive deeper in programmatically generating prompts * [Run your script](/genaiscript/getting-started/running-scripts) from Visual Studio Code.