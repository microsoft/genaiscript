---
title: Cache
sidebar:
    order: 15
description: Learn how LLM requests are cached in scripts to optimize performance and how to manage cache settings.
keywords: cache management, LLM request caching, script performance, cache file customization, cache disabling
---

import { FileTree } from "@astrojs/starlight/components"

LLM requests are cached by default. This means that if a script generates the same prompt for the same model, the cache may be used.

-   the `temperature` is less than 0.5
-   the `top_p` is less than 0.5
-   no [functions](./functions.md) are used as they introduce randomness
-   `seed` is not used

The cache is stored in the `.genaiscript/cache/chat.jsonl` file. You can delete this file to clear the cache.
This file is excluded from git by default.

<FileTree>

-   .genaiscript
    -   cache
        -   chat.jsonl

</FileTree>

## Disabling

You can always disable the cache using the `cache` option in `script`.

```js
script({
    ...,
    cache: false // always off
})
```

Or using the `--no-cache` flag in the CLI.

```sh
npx genaiscript run .... --no-cache
```

## Custom cache file

Use the `cacheName` option to specify a custom cache file name.
The name will be used to create a file in the `.genaiscript/cache` directory.

```js
script({
    ...,
    cacheName: "summary"
})
```

Or using the `--cache-name` flag in the CLI.

```sh
npx genaiscript run .... --cache-name summary
```

<FileTree>

-   .genaiscript
    -   cache
        -   summary.jsonl

</FileTree>

## Programmatic cache

You can instantiate a custom cache object to manage the cache programmatically.

```js
const cache = await workspace.cache("summary")
// write entries
await cache.set("file.txt", "...")
// read entries
const content = await cache.get("file.txt")
// list entries
const entries = await cache.values()
```