---
title: Cache
sidebar:
    order: 15
---

import { FileTree } from "@astrojs/starlight/components"

LLM requests are cached by default. This means that if a script generates the same prompt for the same model, the cache may be used.

-   the `temperature` is less than 0.5
-   the `top_p` is less than 0.5
-   no [functions](./functions.md) are used as they introduce randomness
-   `seed` is not used

The cache is stored in the `.genaiscript/cache/chat.jsonl` file. You can delete this file to clear the cache.
This file is excluded from git by default.

<FileTree>

-   .genaiscript
    -   cache
        -   chat.jsonl

</FileTree>

## Disabling

You can always disable the cache using the `cache` option in `script`.

```js
script({
    ...,
    cache: false // always off
})
```

Or using the `--no-cache` flag in the CLI.

```sh
npx genaiscript run .... --no-cache
```

## Custom cache file

Use the `cacheName` option to specify a custom cache file name.
The name will be used to create a file in the `.genaiscript/cache` directory.

```js
script({
    ...,
    cacheName: "summary"
})
```

Or using the `--cache-name` flag in the CLI.

```sh
npx genaiscript run .... --cache-name summary
```

<FileTree>

-   .genaiscript
    -   cache
        -   summary.jsonl

</FileTree>