---
title: Configuration
description: Set up your LLM connection and authorization with environment variables for seamless integration.
keywords: LLM setup, API configuration, environment variables, secure authorization, LLM integration
sidebar:
    order: 2
---

import { FileTree } from "@astrojs/starlight/components"
import { Steps } from "@astrojs/starlight/components"
import { Tabs, TabItem } from "@astrojs/starlight/components"
import { Image } from "astro:assets"

import insidersSrc from "../../../assets/vscode-insiders.png"
import insidersAlt from "../../../assets/vscode-insiders.png.txt?raw"

import selectLLMSrc from "../../../assets/vscode-select-llm.png"
import selectLLMAlt from "../../../assets/vscode-select-llm.png.txt?raw"

You will need to configure the LLM connection and authorizion secrets.

:::tip
If you do not have access to an LLM, you can use a [local model](#local-models) for inferencing.
:::

## model selection

The model used by the script is configured throught the `model` field in the `script` function.
The model name is formatted as `provider:model-name`, where `provider` is the LLM provider
and the `model-name` is provider specific.

```js 'model: "openai:gpt-4"'
script({
    model: "openai:gpt-4",
})
```

The model can also be overriden from the [cli run command](/genaiscript/reference/cli/run#model).

## `.env` file

GenAIScript uses a `.env` file to store the secrets.

<Steps>

<ol>

<li>

Create or update a `.gitignore` file in the root of your project and make it sure it includes `.env`.
This ensures that you do not accidentally commit your secrets to your source control.

```txt title=".gitignore" ".env"
...
.env
```

</li>

<li>

Create a `.env` file in the root of your project.

<FileTree>

-   .gitignore
-   **.env**
    </FileTree>

</li>

<li>

Update the `.env` file with the configuration information (see below).

</li>

</ol>

</Steps>

:::caution[Do Not Commit Secrets]

The `.env` file should never be commited to your source control!
If the `.gitignore` file is properly configured,
the `.env` file will appear grayed out in Visual Studio Code.

:::

## OpenAI

This provider, `openai`, is the default provider.
It uses the `OPENAI_API_...` environment variables.

<Steps>

<ol>

<li>
    Create a new secret key from the [OpenAI API Keys
    portal](https://platform.openai.com/api-keys).
</li>

<li>

Update the `.env` file with the secret key.

```txt title=".env"
OPENAI_API_KEY=sk_...
```

</li>

<li>

Set the `model` field in `script` to the model you want to use.

```js 'model: "openai:gpt-4"'
script({
    model: "openai:gpt-4",
    ...
})
```

</li>

</ol>

</Steps>

## Azure OpenAI ([reference](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions))

<a id="azure" href=""></a>

The Azure OpenAI provider, `azure` uses the `AZURE_OPENAI_...` environment variables.
You can use a managed identity (recommended) or a API key to authenticate with the Azure OpenAI service.

### Managed Identity (Entra ID)

<Steps>

<ol>

<li>

Open your [Azure OpenAI resource](https://portal.azure.com) and navigate to **Resource Management**, then **Keys and Endpoint**.

</li>

<li>

Update the `.env` file with the endpoint.

```txt title=".env"
AZURE_OPENAI_ENDPOINT=https://....openai.azure.com
```

:::note

Make sure to remove any `AZURE_API_KEY`, `AZURE_OPENAI_API_KEY` entries from `.env` file.

:::

</li>

<li>

Navigate to **deployments** and make sure that you have your LLM deployed and copy the `deployment-id`, you will need it in the script.

</li>

<li>

Update the `model` field in the `script` function to match the model deployment name in your Azure resource.

```js 'model: "azure:deployment-id"'
script({
    model: "azure:deployment-id",
    ...
})
```

</li>

<li>

If you are using the command line, you will need to login with the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/).

```sh
az login
```

If you are using Visual Studio Code, the extension will ask you to sign in.

</li>

</ol>

</Steps>

### API Key

<Steps>

<ol>

<li>

Open your [Azure OpenAI resource](https://portal.azure.com) and navigate to **Resource Management**, then **Keys and Endpoint**.

</li>

<li>

Update the `.env` file with the secret key (**Key 1** or **Key 2**) and the endpoint.

```txt title=".env"
AZURE_OPENAI_API_KEY=...
AZURE_OPENAI_API_ENDPOINT=https://....openai.azure.com
```

</li>

<li>

Open Azure AI Studio, select **Deployments**
and make sure that you have your LLM deployed and copy the `deployment-id`,
you will need it in the script.

</li>

<li>

Update the `model` field in the `script` function to match the model deployment name in your Azure resource.

```js 'model: "azure:deployment-id"'
script({
    model: "azure:deployment-id",
    ...
})
```

</li>

</ol>

</Steps>

## Local Models

There are many projects that allow you to run models locally on your machine,
or in a container.

### LocalAI

[LocalAI](https://localai.io/) act as a drop-in replacement REST API thatâ€™s compatible
with OpenAI API specifications for local inferencing. It uses free Open Source models
and it runs on CPUs.

LocalAI acts as an OpenAI replacement, you can see the [model name mapping](https://localai.io/basics/container/#all-in-one-images)
used in the container, like `gpt-4` is mapped to `phi-2`.

<Steps>

<ol>

<li>

Install Docker. See the [LocalAI documentation](https://localai.io/basics/getting_started/#prerequisites) for more information.

</li>

<li>

Update the `.env` file and set the api type to `localai`.

```txt title=".env" "localai"
OPENAI_API_TYPE=localai
```

</li>

</ol>

</Steps>

### Ollama

[Ollama](https://ollama.ai/) is a desktop application that let you download and run model locally.

Running tools locally may require additional GPU resources depending on the model you are using.

Use the `ollama` provider to access Ollama models.

<Steps>

<ol>

<li>

Start the Ollama application or

```sh
ollama serve
```

</li>

<li>

Update your script to use the `ollama:phi3` model.

```js "ollama:phi3"
script({
    ...,
    model: "ollama:phi3",
})
```

</li>

</ol>

</Steps>

### Llamafile

[https://llamafile.ai/](https://llamafile.ai/) is a single file desktop application
that allows you to run an LLM locally.

The provider is `llamafile` and the model name is ignored.

### Jan, LMStudio, LLaMA.cpp

[Jan](https://jan.ai/), [LMStudio](https://lmstudio.ai/),
[LLaMA.cpp](https://github.com/ggerganov/llama.cpp/tree/master/examples/server)
also allow running models locally or interfacing with other LLM vendors.

<Steps>

<ol>

<li>

Update the `.env` file with the local server information.

```txt title=".env"
OPENAI_API_BASE=http://localhost:...
```

</li>

</ol>

</Steps>

### Model specific environment variables

You can provide different environment variables
for each named model by using the `PROVIDER_MODEL_API_...` prefix or `PROVIDER_API_...` prefix.
The model name is capitalized and
all non-alphanumeric characters are converted to `_`.

This allows to have various sources of LLM computations
for different models. For example, to enable the `ollama:phi3`
model running locally, while keeping the default `openai` model connection information.

```txt title=".env"
OLLAMA_PHI3_API_BASE=http://localhost:11434/v1
```

## VSCode Copilot Insiders

Visual Studio Code **Insiders** provides an
**experimental support** to use your Copilot subscription
to access a LLM inside of Visual Studio Code.

:::caution[Limitations]

-   **This features requires a GitHub Copilot subscription.**
-   **This feature is still a [proposed api](https://github.com/microsoft/vscode/blob/main/src/vscode-dts/vscode.proposed.languageModels.d.ts) and requires the Insiders editor.**
-   The API changes daily and may not be stable.
-   Functions and Images are not supported.
-   The model configuration, temperature, max tokens, may not be available depending on the language model provider.
-   Some scripts may fail or behave differently due to internal alignment prompts from Copilot.

:::

<Steps>

<ol>

<li>

Install [Visual Studio Code **Insiders**](https://code.visualstudio.com/insiders/).
It can be installed side-by-side with the stable version of Visual Studio Code.

</li>

<li>

Open your project in Visual Studio Code Insiders (green icon).

<Image src={insidersSrc} alt={insidersAlt} />

</li>

<li>

Download **genaiscript.insiders.vsix** from the [releases page](https://github.com/microsoft/genaiscript/releases/latest)
to your project folder.

<FileTree>

-   ...
-   **genaiscript.insiders.vsix**

</FileTree>

</li>

<li>

Right click on **genaiscript.insiders.vsix** and select **Install from VSIX**.

</li>

<li>

Add a `.vscode-insiders/argv.json` file to enables loading the proposed APIs for the GenAIScript extension.

```json title=".vscode-insiders/argv.json"
{
    "enable-proposed-api": ["genaiscript.genaiscript-vscode"]
}
```

</li>

<li>

Restart Visual Studio Code Insiders to enable the extension.

</li>

<li>

When you try to run a script, you will be prompted to authorize the Copilot subscription.
After authorizing access, you will see a quick pick dialog to select one of the installed Language Model.

<Image src={selectLLMSrc} alt={selectLLMAlt} />

Note that this dialog is not used once you have a populated `.env` file.

</li>

</ol>

</Steps>

If you are using the Insiders build, make sure to update the **genaiscript.insiders.vsix** regularly
by repeating the steps above. The daily builds of Insiders may contain breaking chagnes that require new builds
of our extension.

## Next steps

Write your [first script](/genaiscript/getting-started/your-first-genai-script).
