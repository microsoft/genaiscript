---
title: Configuration
description: Set up your LLM connection and authorization with environment variables for seamless integration.
keywords: LLM setup, API configuration, environment variables, secure authorization, LLM integration
sidebar:
    order: 2
---

import { FileTree } from "@astrojs/starlight/components"
import { Steps } from "@astrojs/starlight/components"
import { Tabs, TabItem } from "@astrojs/starlight/components"
import { Image } from "astro:assets"

import insidersSrc from "../../../assets/vscode-insiders.png"
import insidersAlt from "../../../assets/vscode-insiders.png.txt?raw"

import selectLLMSrc from "../../../assets/vscode-select-llm.png"
import selectLLMAlt from "../../../assets/vscode-select-llm.png.txt?raw"

You will need to configure the LLM connection and authorizion secrets.

:::tip
If you do not have access to an LLM, you can use a [local model](#local-models) for inferencing.
:::

## model selection

The model used by the script is configured throught the `model` field in the `script` function.
The model name is formatted as `provider:model-name`, where `provider` is the LLM provider
and the `model-name` is provider specific.

```js 'model: "openai:gpt-4"'
script({
    model: "openai:gpt-4",
})
```

The model can also be overriden from the [cli run command](/genaiscript/reference/cli/run#model).

## `.env` file

GenAIScript uses a `.env` file to store the secrets.

<Steps>

<ol>

<li>

Create or update a `.gitignore` file in the root of your project and make it sure it includes `.env`.
This ensures that you do not accidentally commit your secrets to your source control.

```txt title=".gitignore" ".env"
...
.env
```

</li>

<li>

Create a `.env` file in the root of your project.

<FileTree>

-   .gitignore
-   **.env**

</FileTree>

</li>

<li>

Update the `.env` file with the configuration information (see below).

</li>

</ol>

</Steps>

:::caution[Do Not Commit Secrets]

The `.env` file should never be commited to your source control!
If the `.gitignore` file is properly configured,
the `.env` file will appear grayed out in Visual Studio Code.

:::

## OpenAI

This provider, `openai`, is the default provider.
It uses the `OPENAI_API_...` environment variables.

<Steps>

<ol>

<li>
    Create a new secret key from the [OpenAI API Keys
    portal](https://platform.openai.com/api-keys).
</li>

<li>

Update the `.env` file with the secret key.

```txt title=".env"
OPENAI_API_KEY=sk_...
```

</li>

<li>

Set the `model` field in `script` to the model you want to use.

```js 'model: "openai:gpt-4o"'
script({
    model: "openai:gpt-4o",
    ...
})
```

</li>

</ol>

</Steps>


:::tip[Default Model Configuration]

Use `GENAISCRIPT_DEFAULT_MODEL` in your `.env` file to set the default model.

```txt
GENAISCRIPT_DEFAULT_MODEL=openai:gpt-4o
```

:::

## Azure OpenAI
<a id="azure" href=""></a>

The  [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions) provider, `azure` uses the `AZURE_OPENAI_...` environment variables.
You can use a managed identity (recommended) or a API key to authenticate with the Azure OpenAI service.

### Managed Identity (Entra ID)

<Steps>

<ol>

<li>

Open your [Azure OpenAI resource](https://portal.azure.com) 

</li>
<li> 

Navigate to **Access Control**, then **View My Access**. Make sure your
user or service principal has the **Cognitive Services OpenAI User/Contributor** role.
If you get a `401` error, it's typically here that you will fix it.

</li>
<li>
Navigate to **Resource Management**, then **Keys and Endpoint**.
</li>
<li>

Update the `.env` file with the endpoint.

```txt title=".env"
AZURE_OPENAI_ENDPOINT=https://....openai.azure.com
```

:::note

Make sure to remove any `AZURE_API_KEY`, `AZURE_OPENAI_API_KEY` entries from `.env` file.

:::

</li>

<li>

Navigate to **deployments** and make sure that you have your LLM deployed and copy the `deployment-id`, you will need it in the script.

</li>

<li>

Update the `model` field in the `script` function to match the model deployment name in your Azure resource.

```js 'model: "azure:deployment-id"'
script({
    model: "azure:deployment-id",
    ...
})
```

</li>
</ol>

</Steps>

#### Visual Studio Code

Visual Studio Code will ask you to allow using the **Microsoft** account
and then will open a browser where you can choose the user or service principal.

- If you are getting `401` errors after a while, try signing out in the user menu (lower left in Visual Studio Code) and back in.

#### CLI

Login with [Azure CLI](https://learn.microsoft.com/en-us/javascript/api/overview/azure/identity-readme?view=azure-node-latest#authenticate-via-the-azure-cli)
then use the [cli](/genaiscript/reference/cli) as usual.

```sh
az login
```

### API Key

<Steps>

<ol>

<li>

Open your [Azure OpenAI resource](https://portal.azure.com) and navigate to **Resource Management**, then **Keys and Endpoint**.

</li>

<li>

Update the `.env` file with the secret key (**Key 1** or **Key 2**) and the endpoint.

```txt title=".env"
AZURE_OPENAI_API_KEY=...
AZURE_OPENAI_API_ENDPOINT=https://....openai.azure.com
```

</li>

<li>

Open Azure AI Studio, select **Deployments**
and make sure that you have your LLM deployed and copy the `deployment-id`,
you will need it in the script.

</li>

<li>

Update the `model` field in the `script` function to match the model deployment name in your Azure resource.

```js 'model: "azure:deployment-id"'
script({
    model: "azure:deployment-id",
    ...
})
```

</li>

</ol>

</Steps>

## Local Models

There are many projects that allow you to run models locally on your machine,
or in a container.

### LocalAI

[LocalAI](https://localai.io/) act as a drop-in replacement REST API thatâ€™s compatible
with OpenAI API specifications for local inferencing. It uses free Open Source models
and it runs on CPUs.

LocalAI acts as an OpenAI replacement, you can see the [model name mapping](https://localai.io/basics/container/#all-in-one-images)
used in the container, like `gpt-4` is mapped to `phi-2`.

<Steps>

<ol>

<li>

Install Docker. See the [LocalAI documentation](https://localai.io/basics/getting_started/#prerequisites) for more information.

</li>

<li>

Update the `.env` file and set the api type to `localai`.

```txt title=".env" "localai"
OPENAI_API_TYPE=localai
```

</li>

</ol>

</Steps>

### Ollama

[Ollama](https://ollama.ai/) is a desktop application that let you download and run model locally.

Running tools locally may require additional GPU resources depending on the model you are using.

Use the `ollama` provider to access Ollama models.

<Steps>

<ol>

<li>

Start the Ollama application or

```sh
ollama serve
```

</li>

<li>

Update your script to use the `ollama:phi3` model.

```js "ollama:phi3"
script({
    ...,
    model: "ollama:phi3",
})
```

</li>

</ol>

</Steps>

### Llamafile

[https://llamafile.ai/](https://llamafile.ai/) is a single file desktop application
that allows you to run an LLM locally.

The provider is `llamafile` and the model name is ignored.

### Jan, LMStudio, LLaMA.cpp

[Jan](https://jan.ai/), [LMStudio](https://lmstudio.ai/),
[LLaMA.cpp](https://github.com/ggerganov/llama.cpp/tree/master/examples/server)
also allow running models locally or interfacing with other LLM vendors.

<Steps>

<ol>

<li>

Update the `.env` file with the local server information.

```txt title=".env"
OPENAI_API_BASE=http://localhost:...
```

</li>

</ol>

</Steps>

### Model specific environment variables

You can provide different environment variables
for each named model by using the `PROVIDER_MODEL_API_...` prefix or `PROVIDER_API_...` prefix.
The model name is capitalized and
all non-alphanumeric characters are converted to `_`.

This allows to have various sources of LLM computations
for different models. For example, to enable the `ollama:phi3`
model running locally, while keeping the default `openai` model connection information.

```txt title=".env"
OLLAMA_PHI3_API_BASE=http://localhost:11434/v1
```

## Checking your configuration

You can check your configuration by running the `genaiscript info env` [command](/genaiscript/reference/cli).
It will display the current configuration information parsed by GenAIScript.

```sh
genaiscript info env
```

## Next steps

Write your [first script](/genaiscript/getting-started/your-first-genai-script).
